Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Ohm2010,
abstract = {Computer scientists have recently undermined our faith in the privacy- protecting power of anonymization, the name for techniques that protect the privacy of individuals in large databases by deleting information like names and social security numbers. These scientists have demonstrated that they can often “reidentify” or “deanonymize” individuals hidden in anonymized data with astonishing ease. By understanding this research, we realize we have made a mistake, labored beneath a fundamental misunderstanding, which has assured us much less privacy than we have assumed. This mistake pervades nearly every information privacy law, regulation, and debate, yet regulators and legal scholars have paid it scant attention. We must respond to the surprising failure of anonymization, and this Article provides the tools to do so.},
author = {Ohm, Paul},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ohm - 2010 - Broken Promises of Privacy Responding to the Surprising Failure of Anonymization.pdf:pdf},
issn = {00415650},
journal = {UCLA Law Review},
keywords = {Ohm2010},
mendeley-tags = {Ohm2010},
number = {6},
pages = {1701--1777},
title = {{Broken Promises of Privacy: Responding to the Surprising Failure of Anonymization}},
url = {http://www.uclalawreview.org/broken-promises-of-privacy-responding-to-the-surprising-failure-of-anonymization-2/},
volume = {57},
year = {2010}
}
@article{Emam2009,
abstract = {Background: Explicit patient consent requirements in privacy laws can have a negative impact on health research, leading to selection bias and reduced recruitment. Often legislative requirements to obtain consent are waived if the information collected or disclosed is de-identified. Objective: The authors developed and empirically evaluated a new globally optimal de-identification algorithm that satisfies the k-anonymity criterion and that is suitable for health datasets. Design: Authors compared OLA (Optimal Lattice Anonymization) empirically to three existing k-anonymity algorithms, Datafly, Samarati, and Incognito, on six public, hospital, and registry datasets for different values of k and suppression limits. Measurement: Three information loss metrics were used for the comparison: precision, discernability metric, and non-uniform entropy. Each algorithm's performance speed was also evaluated. Results: The Datafly and Samarati algorithms had higher information loss than OLA and Incognito; OLA was consistently faster than Incognito in finding the globally optimal de-identification solution. Conclusions: For the de-identification of health datasets, OLA is an improvement on existing k-anonymity algorithms in terms of information loss and performance. {\textcopyright} 2009 J Am Med Inform Assoc.},
author = {{El Emam}, Khaled and Dankar, Fida Kamal and Issa, Romeo and Jonker, Elizabeth and Amyot, Daniel and Cogo, Elise and Corriveau, Jean Pierre and Walker, Mark and Chowdhury, Sadrul and Vaillancourt, Regis and Roffey, Tyson and Bottomley, Jim},
doi = {10.1197/jamia.M3144},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/El Emam et al. - 2009 - A Globally Optimal k-Anonymity Method for the De-Identification of Health Data.pdf:pdf},
isbn = {1527-974X; 1067-5027},
issn = {10675027},
journal = {Journal of the American Medical Informatics Association},
number = {5},
pages = {670--682},
pmid = {19567795},
title = {{A Globally Optimal k-Anonymity Method for the De-Identification of Health Data}},
volume = {16},
year = {2009}
}
@article{Dwork2013,
abstract = {The Algorithmic Foundations of Differential Privacy},
author = {Dwork, Cynthia and Roth, Aaron},
doi = {10.1561/0400000042},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dwork, Roth - 2013 - The Algorithmic Foundations of Differential Privacy.pdf:pdf},
isbn = {9781601988188},
issn = {1551-305X},
journal = {Foundations and Trends{\textregistered} in Theoretical Computer Science},
keywords = {book},
mendeley-tags = {book},
number = {3-4},
pages = {211--407},
pmid = {21455981},
title = {{The Algorithmic Foundations of Differential Privacy}},
url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-theoretical-computer-science/TCS-042},
volume = {9},
year = {2013}
}
@article{Wigan2013,
abstract = {Businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons. The Web extra at http://youtu.be/TvXoQhrrGzg is a video in which author Marcus Wigan expands on his article "Big Data's Big Unintended Consequences" and discusses how businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons.},
author = {Wigan, Marcus R. and Clarke, Roger},
doi = {10.1109/MC.2013.195},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wigan, Clarke - 2013 - Big data's big unintended consequences.pdf:pdf},
isbn = {0018-9162},
issn = {00189162},
journal = {Computer},
keywords = {big data,data,policy,privacy,private data commons,social impact},
number = {6},
pages = {46--53},
publisher = {IEEE Computer Society},
title = {{Big data's big unintended consequences}},
volume = {46},
year = {2013}
}
@article{Li2007,
abstract = {The k-anonymity privacy requirement for publishing microdata requires that each equivalence class (i.e., a set of records that are indistinguishable from each other with respect to certain "identifying" attributes) contains at least k records. Recently, several authors have recognized that k-anonymity cannot prevent attribute disclosure. The notion of l-diversity has been proposed to address this; l-diversity requires that each equivalence class has at least l well-represented values for each sensitive attribute. In this paper we show that l-diversity has a number of limitations. In particular, it is neither necessary nor sufficient to prevent attribute disclosure. We propose a novel privacy notion called t-closeness, which requires that the distribution of a sensitive attribute in any equivalence class is close to the distribution of the attribute in the overall table (i.e., the distance between the two distributions should be no more than a threshold t). We choose to use the earth mover distance measure for our t-closeness requirement. We discuss the rationale for t-closeness and illustrate its advantages through examples and experiments.},
author = {Ninghui, Li and Tiancheng, Li and Venkatasubramanian, Suresh},
doi = {10.1109/ICDE.2007.367856},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ninghui, Tiancheng, Venkatasubramanian - 2007 - t-Closeness Privacy beyond k-anonymity and l-diversity.pdf:pdf},
isbn = {1424408032},
issn = {10844627},
journal = {Proceedings - International Conference on Data Engineering},
number = {3},
pages = {106--115},
title = {{t-Closeness: Privacy beyond k-anonymity and l-diversity}},
year = {2007}
}
@article{Brickell2008,
abstract = {Re-identification is a major privacy threat to public datasets containing individual records. Many privacy protection algorithms rely on generalization and suppression of "quasi-identifier" attributes such as ZIP code and birthdate. Their objective is usually syntactic sanitization: for example, k-anonymity requires that each "quasi-identifier" tuple appear in at least k records, while l-diversity requires that the distribution of sensitive attributes for each quasi-identifier have high entropy. The utility of sanitized data is also measured syntactically, by the number of generalization steps applied or the number of records with the same quasi-identifier. In this paper, we ask whether generalization and suppression of quasi-identifiers offer any benefits over trivial sanitization which simply separates quasi-identifiers from sensitive attributes. Previous work showed that k-anonymous databases can be useful for data mining, but k-anonymization does not guarantee any privacy. By contrast, we measure the tradeoff between privacy (how much can the adversary learn from the sanitized records?) and utility, measured as accuracy of data-mining algorithms executed on the same sanitized records. For our experimental evaluation, we use the same datasets from the UCI machine learning repository as were used in previous research on generalization and suppression. Our results demonstrate that even modest privacy gains require almost complete destruction of the data-mining utility. In most cases, trivial sanitization provides equivalent utility and better privacy than k-anonymity, l-diversity, and similar methods based on generalization and suppression.},
author = {Brickell, Justin and Shmatikov, Vitaly},
doi = {10.1145/1401890.1401904},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brickell, Shmatikov - 2008 - The cost of privacy destruction of data-mining utility in anonymized data publishing.pdf:pdf},
isbn = {978-1-60558-193-4},
journal = {Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining},
keywords = {anonymity,data mining,privacy,utility},
pages = {70--78},
title = {{The Cost of Privacy: Destruction of Data-Mining Utility in Anonymized Data Publishing}},
url = {http://doi.acm.org/10.1145/1401890.1401904},
year = {2008}
}
@article{OKeefe2018,
abstract = {{\textcopyright} The Author 2017. Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved. Objective: Recent growth in the number of population health researchers accessing detailed datasets, either on their own computers or through virtual data centers, has the potential to increase privacy risks. In response, a checklist for identifying and reducing privacy risks in population health analysis outputs has been proposed for use by researchers themselves. In this study we explore the usability and reliability of such an approach by investigating whether different users identify the same privacy risks on applying the checklist to a sample of publications. Methods: The checklist was applied to a sample of 100 academic population health publications distributed among 5 readers. Cohen's j was used to measure interrater agreement. Results: Of the 566 instances of statistical output types found in the 100 publications, the most frequently occurring were counts, summary statistics, plots, and model outputs. Application of the checklist identified 128 outputs (22.6{\%}) with potential privacy concerns. Most of these were associated with the reporting of small counts. Among these identified outputs, the readers found no substantial actual privacy concerns when context was taken into account. Interrater agreement for identifying potential privacy concerns was generally good. Conclusion: This study has demonstrated that a checklist can be a reliable tool to assist researchers with anonymizing analysis outputs in population health research. This further suggests that such an approach may have the potential to be developed into a broadly applicable standard providing consistent confidentiality protection across multiple analyses of the same data.},
author = {O'Keefe, Christine M. and Ickowicz, Adrien and Churches, Tim and Westcott, Mark and O'Sullivan, Maree and Khan, Atikur},
doi = {10.1093/jamia/ocx129},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Keefe et al. - 2018 - Assessing privacy risks in population health publications using a checklist-based approach.pdf:pdf},
isbn = {1527-974X (Electronic)1067-5027 (Linking)},
issn = {1527974X},
journal = {Journal of the American Medical Informatics Association},
keywords = {Biomedical research,Confidentiality,Data anonymization,Health services research,Privacy},
number = {3},
pages = {315--320},
pmid = {29136182},
title = {{Assessing privacy risks in population health publications using a checklist-based approach}},
volume = {25},
year = {2018}
}
@article{OKeefe2017,
abstract = {Objective: Online data centers (ODCs) are becoming increasingly popular for making health-related data available for research. Such centers provide good privacy protection during analysis by trusted researchers, but privacy concerns may still remain if the system outputs are not sufficiently anonymized. In this article, we propose a method for anonymizing analysis outputs from ODCs for publication in academic literature.Methods: We use as a model system the Secure Unified Research Environment, an online computing system that allows researchers to access and analyze linked health-related data for approved studies in Australia. This model system suggests realistic assumptions for an ODC that, together with literature and practice reviews, inform our solution design.Results: We propose a two-step approach to anonymizing analysis outputs from an ODC. A data preparation stage requires data custodians to apply some basic treatments to the dataset before making it available. A subsequent output anonymization stage requires researchers to use a checklist at the point of downloading analysis output. The checklist assists researchers with highlighting potential privacy concerns, then applying appropriate anonymization treatments.Conclusion: The checklist can be used more broadly in health care research, not just in ODCs. Ease of online publication as well as encouragement from journals to submit supplementary material are likely to increase both the volume and detail of analysis results publicly available, which in turn will increase the need for approaches such as the one suggested in this paper.},
author = {O'Keefe, Christine M. and Westcott, Mark and O'sullivan, Maree and Ickowicz, Adrien and Churches, Tim},
doi = {10.1093/jamia/ocw152},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Keefe et al. - 2017 - Anonymization for outputs of population health and health services research conducted via an online data center.pdf:pdf},
issn = {1527974X},
journal = {Journal of the American Medical Informatics Association},
keywords = {Biomedical research,Conﬁdentiality,Data anonymization,Privacy},
number = {3},
pages = {544--549},
title = {{Anonymization for outputs of population health and health services research conducted via an online data center}},
volume = {24},
year = {2017}
}
@article{Zang2011,
abstract = {We examine a very large-scale data set of more than 30 bil- lion call records made by 25 million cell phone users across all 50 states of the US and attempt to determine to what extent anonymized location data can reveal private user in- formation. Our approach is to infer, from the call records, the “top N” locations for each user and correlate this in- formation with publicly-available side information such as census data. For example, the measured “top 2” locations likely correspond to home and work locations, the “top 3” to home, work, and shopping/school/commute path locations. We consider the cases where those “top N” locations are measured with different levels of granularity, ranging from a cell sector to whole cell, zip code, city, county and state. We then compute the anonymity set, namely the number of users uniquely identified by a given set of “top N” locations at different granularity levels. We find that the “top 1” location does not typically yield small anonymity sets. However, the top 2 and top 3 loca- tions do, certainly at the sector or cell-level granularity. We consider a variety of different factors that might impact the size of the anonymity set, for example the distance between the “top N” locations or the geographic environment (rural vs urban). We also examine to what extent specific side in- formation, in particular the size of the user's social network, decrease the anonymity set and therefore increase risks to privacy. Our study shows that sharing anonymized location data will likely lead to privacy risks and that, at a mini- mum, the data needs to be coarse in either the time domain (meaning the data is collected over short periods of time, in which case inferring the top N locations reliably is difficult) or the space domain (meaning the data granularity is strictly higher than the cell level). In both cases, the utility of the anonymized location data will be decreased, potentially by a significant amount. ∗Part},
author = {Zang, H. and Bolot, J.},
doi = {10.1145/2030613.2030630},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zang, Bolot - 2011 - Anonymization of Location Data Does Not Work A Large-Scale Measurement Study.pdf:pdf},
isbn = {9781450304924},
journal = {Proceedings of the 17th annual international conference on Mobile computing and networking, ACM.},
keywords = {cellular data,k -anonymity,location,privacy},
pages = {145--156},
title = {{Anonymization of Location Data Does Not Work : A Large-Scale Measurement Study}},
year = {2011}
}
@article{LeFevre2005,
abstract = {A number of organizations publish microdata for purposes such as public health and demographic research. Although attributes that clearly identify individuals, such as Name and Social Security Number, are generally removed, these databases can sometimes be joined with other public databases on attributes such as Zipcode, Sex, and Birthdate to re-identify individuals who were supposed to remain anonymous. "Joining" attacks are made easier by the availability of other, complementary, databases over the Internet.K-anonymization is a technique that prevents joining attacks by generalizing and/or suppressing portions of the released microdata so that no individual can be uniquely distinguished from a group of size k. In this paper, we provide a practical framework for implementing one model of k-anonymization, called full-domain generalization. We introduce a set of algorithms for producing minimal full-domain generalizations, and show that these algorithms perform up to an order of magnitude faster than previous algorithms on two real-life databases.Besides full-domain generalization, numerous other models have also been proposed for k-anonymization. The second contribution in this paper is a single taxonomy that categorizes previous models and introduces some promising new alternatives.},
author = {LeFevre, Kristen and DeWitt, David J D.J. and Ramakrishnan, Raghu},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeFevre, DeWitt, Ramakrishnan - 2005 - Incognito efficient full-domain K-anonymity.pdf:pdf},
journal = {SIGMOD '05: Proceedings of the 2005 ACM SIGMOD international conference on Management of data},
keywords = {LeFevre2005},
mendeley-tags = {LeFevre2005},
pages = {49--60},
title = {{Incognito: efficient full-domain K-anonymity}},
url = {http://portal.acm.org/citation.cfm?doid=1066157.1066164{\%}5Cnhttp://dl.acm.org/citation.cfm?id=1066164{\%}5Cnhttp://doi.acm.org/10.1145/1066157.1066164{\%}5Cnhttp://portal.acm.org/citation.cfm?id=1066164},
year = {2005}
}
@article{Liu2018,
abstract = {{\textcopyright} 2013 IEEE. This paper surveys the current research status of location privacy issues in mobile applications. The survey spans five aspects of study: the definition of location privacy, attacks and adversaries, mechanisms to preserve the privacy of locations, location privacy metrics, and the current status of location-based applications. Through this comprehensive review, all the interrelated aspects of location privacy are integrated into a unified framework. Additionally, the current research progress in each area is reviewed individually, and the links between existing academic research and its practical applications are identified. This in-depth analysis of the current state-of-play in location privacy is designed to provide a solid foundation for future studies in the field.},
author = {Liu, Bo and Zhou, Wanlei and Zhu, Tianqing and Gao, Longxiang and Xiang, Yong},
doi = {10.1109/ACCESS.2018.2822260},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2018 - Location Privacy and Its Applications A Systematic Study.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Anonymization,Deidentification,GPS,Location,Location privacy,Privacy,location-based service,mobile applications},
mendeley-tags = {Anonymization,Deidentification,GPS,Location,Privacy},
pages = {17606--17624},
title = {{Location Privacy and Its Applications: A Systematic Study}},
volume = {6},
year = {2018}
}
@incollection{Dwork2016,
abstract = {We continue a line of research initiated in Dinur and Nissim (2003); Dwork and Nissim (2004); Blum et al. (2005) on privacy-preserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user. Previous work focused on the case of noisy sums, in which f = i g(x i), where x i denotes the ith row of the database and g maps database rows to [0, 1]. We extend the study to general functions f , proving that privacy can be preserved by calibrating the standard deviation of the noise according to the sensitivity of the function f . Roughly speaking, this is the amount that any single argument to f can change its output. The new analysis shows that for several particular applications substantially less noise is needed than was previously understood to be the case. The first step is a very clean definition of privacy—now known as differential privacy— and measure of its loss. We also provide a set of tools for designing and combining differentially private algorithms, permitting the construction of complex differentially private analytical tools from simple differentially private primitives. Finally, we obtain separation results showing the increased value of interactive sta-tistical release mechanisms over non-interactive ones.},
author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
booktitle = {Journal of Privacy and Confidentiality},
doi = {10.1007/11681878_14},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dwork et al. - 2006 - Calibrating Noise to Sensitivity in Private Data Analysis.pdf:pdf},
number = {3},
pages = {265--284},
title = {{Calibrating Noise to Sensitivity in Private Data Analysis}},
url = {http://repository.cmu.edu/cgi/viewcontent.cgi?article=1139{\&}context=jpc http://link.springer.com/10.1007/11681878{\_}14},
volume = {7},
year = {2006}
}
@article{Machanavajjhala2007,
abstract = {Background The ability to generate raw biomedical scientific data is outpacing the comparable availability of compute capabilities for data analysis and knowledge discovery [1]. Within existing clinical environments, where much of these data are generated, systems are primarily architected for high reliability, availability and persistence, and not for rapidly evolving research-driven computationally intensive analytics. Transitioning access of raw data to systems that support these researcher analysis needs remains rare and largely isolated due to concerns with data security and privacy coupled with a lack of established service-oriented data infrastructures. Significance: Processing biomedical data While this project has the potential to identify new capabilities for data analytics within a broad range of data science disciplines, including finance, insurance, and engineering, the primary focus for this project will be medical information. With hospitals currently generating tremendous amounts of patient data including physician notes, laboratory tests, and billing summaries, a significant barrier to utilization of these data is the lack of methodologies to support the chain of custody for the storage, transfer, and derivative analytics when using compute resources external to data origins [2]. Traditionally, what data has been permitted transfer to secondary locations in biomedicine has been in highly aggregated or de-identified forms, which can severely limit its utility [3], and has been criticized as being insufficient as a solely technical solution due to weak points in data provenance through necessary processing and re/delinkage of data [4], [5].},
author = {Machanavajjhala, Ashwin and Kifer, Daniel and Gehrke, Johannes and Venkitasubramaniam, Muthuramakrishnan},
doi = {10.1145/1217299.1217302},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Machanavajjhala et al. - 2007 - l-diversity.pdf:pdf},
isbn = {9781605584959},
issn = {15564681},
journal = {ACM Transactions on Knowledge Discovery from Data},
number = {1},
pages = {3--es},
title = {l-diversity},
url = {http://portal.acm.org/citation.cfm?doid=1217299.1217302},
volume = {1},
year = {2007}
}
@article{Malin2006,
abstract = {In this paper, we investigate how location access patterns influence the re-identification of seemingly anonymous data. In the real world, individuals visit different locations that gather similar information. For instance, multiple hospitals collect health information on the same patient. To protect anonymity for research purposes, hospitals share sensitive data, such as DNA sequences, stripped of explicit identifiers. Separately, for administrative functions, identified data, stripped of DNA, is made available. On a hospital by hospital basis, each pair of DNA and identified databases appears unlinkable, however, links can be established when multiple locations' database are studied. This problem, known as trail re-identification, is a generalized phenomenon and occurs because an individual's location access pattern can be matched across the shared databases. Data holders can not exchange data to find and suppress trails that would be re-identified. Thus, it is important to assess the re-identification risk in a system in order to develop techniques to mitigate it. In this research, we evaluate several real world datasets and observe trail re-identification is related to the number of people to places. To study this phenomenon in more detail, we develop a generative model for location access patterns that simulates observed behavior. We evaluate trail re-identification risk in a range of simulated patterns and our findings suggest that the skew of the distribution of people to places is one of the main factors that drives trail re-identification.},
author = {Malin, Bradley and Airoldi, Edoardo},
doi = {10.1007/11957454_24},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Malin, Airoldi - 2006 - The effects of location access behavior on re-identification risk in a distributed environment.pdf:pdf},
isbn = {3540687904},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {413--429},
title = {{The effects of location access behavior on re-identification risk in a distributed environment}},
volume = {4258 LNCS},
year = {2006}
}
@inproceedings{Abadi2016,
abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
archivePrefix = {arXiv},
arxivId = {1607.00133},
author = {Abadi, Mart{\'{i}}n and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
booktitle = {23rd ACM Conference on Computer and Communications Security},
doi = {10.1145/2976749.2978318},
eprint = {1607.00133},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - 2016 - Deep Learning with Differential Privacy.pdf:pdf},
isbn = {9781450341394},
issn = {9781450321389},
title = {{Deep Learning with Differential Privacy}},
year = {2016}
}
@article{DeMontjoye2013,
abstract = {We study fifteen months of human mobility data for one and a half million individuals and find that human mobility traces are highly unique. In fact, in a dataset where the location of an individual is specified hourly, and with a spatial resolution equal to that given by the carrier's antennas, four spatio-temporal points are enough to uniquely identify 95{\%} of the individuals. We coarsen the data spatially and temporally to find a formula for the uniqueness of human mobility traces given their resolution and the available outside information. This formula shows that the uniqueness of mobility traces decays approximately as the 1/10 power of their resolution. Hence, even coarse datasets provide little anonymity. These findings represent fundamental constraints to an individual's privacy and have important implications for the design of frameworks and institutions dedicated to protect the privacy of individuals.},
author = {{De Montjoye}, Yves-Alexandre and Hidalgo, C{\'{e}}sar A. and Verleysen, Michel and Blondel, Vincent D.},
doi = {10.1038/srep01376},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Montjoye et al. - 2013 - Unique in the Crowd The privacy bounds of human mobility.pdf:pdf},
isbn = {2045-2322},
issn = {20452322},
journal = {Scientific Reports},
pages = {1--5},
pmid = {23524645},
title = {{Unique in the Crowd: The privacy bounds of human mobility}},
volume = {3},
year = {2013}
}
@article{Malin2007,
abstract = {Objective: Health care organizations must preserve a patient's anonymity when disclosing personal data. Traditionally, patient identity has been protected by stripping identifiers from sensitive data such as DNA. However, simple automated methods can re-identify patient data using public information. In this paper, we present a solution to prevent a threat to patient anonymity that arises when multiple health care organizations disclose data. In this setting, a patient's location visit pattern, or "trail", can re-identify seemingly anonymous DNA to patient identity. This threat exists because health care organizations (1) cannot prevent the disclosure of certain types of patient information and (2) do not know how to systematically avoid trail re-identification. In this paper, we develop and evaluate computational methods that health care organizations can apply to disclose patient-specific DNA records that are impregnable to trail re-identification. Methods and materials: To prevent trail re-identification, we introduce a formal model called k-unlinkability, which enables health care administrators to specify different degrees of patient anonymity. Specifically, k-unlinkability is satisfied when the trail of each DNA record is linkable to no less than k identified records. We present several algorithms that enable health care organizations to coordinate their data disclosure, so that they can determine which DNA records can be shared without violating k-unlinkability. We evaluate the algorithms with the trails of patient populations derived from publicly available hospital discharge databases. Algorithm efficacy is evaluated using metrics based on real world applications, including the number of suppressed records and the number of organizations that disclose records. Results: Our experiments indicate that it is unnecessary to suppress all patient records that initially violate k-unlinkability. Rather, only portions of the trails need to be suppressed. For example, if each hospital discloses 100{\%} of its data on patients diagnosed with cystic fibrosis, then 48{\%} of the DNA records are 5-unlinkable. A na{\"{i}}ve solution would suppress the 52{\%} of the DNA records that violate 5-unlinkability. However, by applying our protection algorithms, the hospitals can disclose 95{\%} of the DNA records, all of which are 5-unlinkable. Similar findings hold for all populations studied. Conclusion: This research demonstrates that patient anonymity can be formally protected in shared databases. Our findings illustrate that significant quantities of patient-specific data can be disclosed with provable protection from trail re-identification. The configurability of our methods allows health care administrators to quantify the effects of different levels of privacy protection and formulate policy accordingly. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Malin, Bradley},
doi = {10.1016/j.artmed.2007.04.002},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Malin - 2007 - A computational model to protect patient data from location-based re-identification.pdf:pdf},
isbn = {0933-3657},
issn = {09333657},
journal = {Artificial Intelligence in Medicine},
keywords = {Confidentiality,Databases,Distributed systems,Electronic medical records,Genomics,Graphical models,Privacy},
number = {3},
pages = {223--239},
pmid = {17544262},
title = {{A computational model to protect patient data from location-based re-identification}},
volume = {40},
year = {2007}
}
@article{ElEmam2011,
abstract = {BACKGROUND: Privacy legislation in most jurisdictions allows the disclosure of health data for secondary purposes without patient consent if it is de-identified. Some recent articles in the medical, legal, and computer science literature have argued that de-identification methods do not provide sufficient protection because they are easy to reverse. Should this be the case, it would have significant and important implications on how health information is disclosed, including: (a) potentially limiting its availability for secondary purposes such as research, and (b) resulting in more identifiable health information being disclosed. Our objectives in this systematic review were to: (a) characterize known re-identification attacks on health data and contrast that to re-identification attacks on other kinds of data, (b) compute the overall proportion of records that have been correctly re-identified in these attacks, and (c) assess whether these demonstrate weaknesses in current de-identification methods.$\backslash$n$\backslash$nMETHODS AND FINDINGS: Searches were conducted in IEEE Xplore, ACM Digital Library, and PubMed. After screening, fourteen eligible articles representing distinct attacks were identified. On average, approximately a quarter of the records were re-identified across all studies (0.26 with 95{\%} CI 0.046-0.478) and 0.34 for attacks on health data (95{\%} CI 0-0.744). There was considerable uncertainty around the proportions as evidenced by the wide confidence intervals, and the mean proportion of records re-identified was sensitive to unpublished studies. Two of fourteen attacks were performed with data that was de-identified using existing standards. Only one of these attacks was on health data, which resulted in a success rate of 0.00013.$\backslash$n$\backslash$nCONCLUSIONS: The current evidence shows a high re-identification rate but is dominated by small-scale studies on data that was not de-identified according to existing standards. This evidence is insufficient to draw conclusions about the efficacy of de-identification methods.},
author = {{El Emam}, Khaled and Jonker, Elizabeth and Arbuckle, Luk and Malin, Bradley},
doi = {10.1371/journal.pone.0028071},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/El Emam et al. - 2011 - A systematic review of re-identification attacks on health data.pdf:pdf},
isbn = {1932-6203; 1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {12},
pmid = {22164229},
title = {{A systematic review of re-identification attacks on health data}},
volume = {6},
year = {2011}
}
@article{Li2011,
abstract = {This paper aims at answering the following two questions in privacy-preserving data analysis and publishing: What formal privacy guarantee (if any) does {\$}k{\$}-anonymization provide? How to benefit from the adversary's uncertainty about the data? We have found that random sampling provides a connection that helps answer these two questions, as sampling can create uncertainty. The main result of the paper is that {\$}k{\$}-anonymization, when done "safely", and when preceded with a random sampling step, satisfies {\$}(\backslashepsilon,\backslashdelta){\$}-differential privacy with reasonable parameters. This result illustrates that "hiding in a crowd of {\$}k{\$}" indeed offers some privacy guarantees. This result also suggests an alternative approach to output perturbation for satisfying differential privacy: namely, adding a random sampling step in the beginning and pruning results that are too sensitive to change of a single tuple. Regarding the second question, we provide both positive and negative results. On the positive side, we show that adding a random-sampling pre-processing step to a differentially-private algorithm can greatly amplify the level of privacy protection. Hence, when given a dataset resulted from sampling, one can utilize a much large privacy budget. On the negative side, any privacy notion that takes advantage of the adversary's uncertainty likely does not compose. We discuss what these results imply in practice.},
archivePrefix = {arXiv},
arxivId = {1101.2604},
author = {Li, Ninghui and Qardaji, Wahbeh and Su, Dong},
doi = {10.1145/2414456.2414474},
eprint = {1101.2604},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Qardaji, Su - 2011 - On Sampling, Anonymization, and Differential Privacy Or, k-Anonymization Meets Differential Privacy.pdf:pdf},
isbn = {9781450316484},
keywords = {all or part of,anonymization,data privacy,differential privacy,is granted without fee,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,this work for},
title = {{On Sampling, Anonymization, and Differential Privacy: Or, k-Anonymization Meets Differential Privacy}},
url = {http://arxiv.org/abs/1101.2604},
year = {2011}
}
@article{Zhang2007,
abstract = {The problem of information disclosure has attracted much interest from the research community in recent years. When disclosing information, the challenge is to provide as much information as possible (optimality) while guaranteeing a desired safety property for privacy (such as l-diversity). A typical disclosure algorithm uses a sequence of disclosure schemas to output generalizations in the nonincreasing order of data utility; the algorithm releases the first generalization that satisfies the safety property. In this paper, we assert that the desired safety property cannot always be guaranteed if an adversary has the knowledge of the underlying disclosure algorithm. We propose a model for the additional information disclosed by an algorithm based on the definition of deterministic disclosure function (DDF), and provide definitions of p-safe and p-optimal DDFs. We give an analysis for the complexity to compute a p-optimal DDF. We show that deciding whether a DDF is p-optimal is an NP-hard problem, and only under specific conditions, we can solve the problem in polynomial time with respect to the size of the set of all possible database instances and the length of the disclosure generalization sequence. We then consider the problem of microdata disclosure and the safety condition of l-diversity. We relax the notion of p-optimality to weak p-optimality, and develop a weak p-optimal algorithm which is polynomial in the size of the original table and the length of the generalization sequence.},
author = {Zhang, L. and Jajodia, Sushil and Brodsky, A.},
doi = {10.1145/1315245.1315316},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Jajodia, Brodsky - 2007 - Information disclosure under realistic assumptions Privacy versus optimality.pdf:pdf},
isbn = {9781595937032},
issn = {15437221},
journal = {Computer and Communications Security},
keywords = {assumptions,ormation disclosure under realistic,privacy versus optimality},
pages = {573--583},
title = {{Information disclosure under realistic assumptions: Privacy versus optimality}},
url = {http://dblp.uni-trier.de/db/conf/ccs/ccs2007.html{\#}ZhangJB07},
year = {2007}
}
@article{Sweeny2002,
abstract = {Consider a data holder, such as a hospital or a bank, that has a privately held collection of person-specific, field structured data. Suppose the data holder wants to share a version of the data with researchers. How can a data holder release a version of its private data with scientific guarantees that the individuals who are the subjects of the data cannot be re-identified while the data remain practically useful? The solution provided in this paper includes a formal protection model named k-anonymity and a set of accompanying policies for deployment. A release provides k-anonymity protection if the information for each person contained in the release cannot be distinguished from at least k-1 individuals whose information also appears in the release. This paper also examines re-identification attacks that can be realized on releases that adhere to k-anonymity unless accompanying policies are respected. The k-anonymity protection model is important because it forms the basis on which the real-world systems known as Datafly, µ-Argus and k-Similar provide guarantees of privacy protection.},
author = {Sweeney, Latanya},
doi = {10.1142/S0218488502001648},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sweeney - 2002 - k-anonymity a model for protecting privacy.pdf:pdf},
journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
keywords = {data anonymity,data fusion,data privacy,privacy,re-identification},
number = {05},
pages = {557--570},
title = {k-anonymity: a model for protecting privacy},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218488502001648},
volume = {10},
year = {2002}
}
@article{Dwork2006,
abstract = {. In 1977 Dalenius articulated a desideratum for statistical databases: nothing about an individual should be learnable from the database that cannot be learned without access to the database. We give a general impossibility result showing that a formalization of Dalenius' goal along the lines of semantic security cannot be achieved. Contrary to intuition, a variant of the result threatens the privacy even of someone not in the database. This state of affairs suggests a new measure, differential privacy, which, intuitively, captures the increased risk to one's privacy incurred by participating in a database. The techniques developed in a sequence of papers [8, 13, 3], culminating in those described in [12], can achieve any desired level of privacy under this measure. In many cases, extremely accurate information about the database can be provided while simultaneously ensuring very high levels of privacy. 1},
author = {Dwork, Cynthia},
doi = {10.1007/11787006_1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dwork - 2006 - Differential privacy.pdf:pdf},
journal = {Proceedings of the 33rd International Colloquium on Automata, Languages and Programming},
pages = {1--12},
title = {{Differential privacy}},
year = {2006}
}
