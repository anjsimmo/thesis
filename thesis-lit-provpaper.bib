Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{DeNies2013,
abstract = {Data provenance is defined as information about entities, activities and people producing or modifying a piece of data. On theWeb, the interchange of standardized provenance of (linked) data is an essential step towards establishing trust [2]. One mechanism to track (part of) the provenance of data, is through the use of version control systems (VCS), such as Git. These systems are widely used to facilitate collaboration primarily for both code and data. Here, we describe a system to expose the provenance stored in VCS in a new standard Web-native format: W3C PROV [4]. This enables the easy publication of VCS provenance on the Web and subsequent integration with other systems that make use of PROV. The system is exposed as a RESTful Web service, which allows integration into user-friendly tools, such as browser plugins. 1},
author = {{De Nies}, Tom and Magliacane, Sara and Verborgh, Ruben and Coppens, Sam and Groth, Paul and Mannens, Erik and {Van De Walle}, Rik},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Nies et al. - 2013 - Git2PROV Exposing version control system content as W3C PROV.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
pages = {125--128},
title = {{Git2PROV: Exposing version control system content as W3C PROV}},
volume = {1035},
year = {2013}
}
@book{Widom2000,
abstract = {We consider the view data lineage problem in a warehousing en vironment. For a given data item in a materialized warehouse view, we want to identify the set of source data items that produced the view item. We formally define the lineage problem, develop lineage tracing algorithms for relational views with aggregation, and propose mechanisms for performing consistent lineage tracing in a multi-source data warehousing environment. Our results can form the basis of a tool that allows analysts to browse warehouse data, select view tuples of interest, then "drill-through" to examine the exact source tuples that produced the view tuples of interest.},
author = {Cui, Yingwei and Widom, Jennifer and Wiener, Janet L.},
booktitle = {ACM Transactions on Database Systems},
doi = {10.1145/357775.357777},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cui, Widom, Wiener - 2000 - Tracing the lineage of view data in a warehousing environment.pdf:pdf},
isbn = {3060296103},
issn = {03625915},
keywords = {database,lineage,provenance},
mendeley-tags = {database,lineage,provenance},
number = {2},
pages = {179--227},
title = {{Tracing the lineage of view data in a warehousing environment}},
url = {http://portal.acm.org/citation.cfm?doid=357775.357777},
volume = {25},
year = {2000}
}
@article{Baker2016,
abstract = {A Nature survey lifts the lid on how researchers view the ‘crisis' rocking science and what they think will help.},
author = {Baker, Monya and Penny, Dan},
doi = {10.1038/533452A},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baker, Penny - 2016 - Is there a reproducibility crisis.pdf:pdf},
isbn = {1476-4687},
issn = {14764687},
journal = {Nature},
keywords = {Folder - scientific{\_}reproducability},
mendeley-tags = {Folder - scientific{\_}reproducability},
month = {may},
number = {7604},
pages = {452--454},
pmid = {27225100},
title = {{Is there a reproducibility crisis?}},
url = {http://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970},
volume = {533},
year = {2016}
}
@article{Oliveira2017,
abstract = {Data provenance is a fundamental concept in scientific experimentation. However, for their proper understanding and use, efficient and user-friendly mechanisms are needed. Research in software visualization, ontologies and complex networks can help in this process. This paper presents a framework to assist in the understanding and use of data provenance using visualization techniques, ontologies and complex networks. The framework capture the provenance data and generates new information using ontologies and provenance graph analysis. The graph is analyzed through complex networks techniques and provide some metrics to help in each node analyzes. The visualization presents and highlights the inferences and results. The framework was used in the E-SECO scientific ecosystem to support the scientific experimentation.},
author = {Oliveira, Weiner and Ambr{\'{o}}sio, Lenitta M. and Braga, Regina and Str{\"{o}}ele, Victor and David, Jos{\'{e}} Maria and Campos, Fernanda},
doi = {10.1016/j.procs.2017.05.216},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oliveira et al. - 2017 - A Framework for Provenance Analysis and Visualization.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Complex Network,E-science,Provenance,Visualization,prov,provenance,usability,visualisation},
mendeley-tags = {prov,provenance,usability,visualisation},
pages = {1592--1601},
title = {{A Framework for Provenance Analysis and Visualization}},
volume = {108},
year = {2017}
}
@article{Pasquier2017,
abstract = {In the last few decades, data-driven methods have come to dominate many fields of scientific inquiry. Open data and open-source software have enabled the rapid implementation of novel methods to manage and analyze the growing flood of data. However, it has become apparent that many scientific fields exhibit distressingly low rates of reproducibility. Although there are many dimensions to this issue, we believe that there is a lack of formalism used when describing end-to-end published results, from the data source to the analysis to the final published results. Even when authors do their best to make their research and data accessible, this lack of formalism reduces the clarity and efficiency of reporting, which contributes to issues of reproducibility. Data provenance aids both reproducibility through systematic and formal records of the relationships among data sources, processes, datasets, publications and researchers. Reproducibility},
author = {Pasquier, Thomas and Lau, Matthew K. and Trisovic, Ana and Boose, Emery R. and Couturier, Ben and Crosas, Merc{\`{e}} and Ellison, Aaron M. and Gibson, Valerie and Jones, Chris R. and Seltzer, Margo},
doi = {10.1038/sdata.2017.114},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pasquier et al. - 2017 - If these data could talk.pdf:pdf},
issn = {20524463},
journal = {Scientific Data},
keywords = {Automation,Open,Pipeline,Process,Prov,Provenance,Reproducibility,Scientific workflows,Workflow},
mendeley-tags = {Automation,Open,Pipeline,Process,Prov,Provenance,Reproducibility,Scientific workflows,Workflow},
pages = {1--5},
title = {{If these data could talk}},
volume = {4},
year = {2017}
}
@article{Moody2009,
abstract = {Visual notations form an integral part of the language of software engineering (SE). Yet historically, SE researchers and notation designers have ignored or undervalued issues of visual representation. In evaluating and comparing notations, details of visual syntax are rarely discussed. In designing notations, the majority of effort is spent on semantics, with graphical conventions largely an afterthought. Typically, no design rationale, scientific or otherwise, is provided for visual representation choices. While SE has developed mature methods for evaluating and designing semantics, it lacks equivalent methods for visual syntax. This paper defines a set of principles for designing cognitively effective visual notations: ones that are optimized for human communication and problem solving. Together these form a design theory, called the Physics of Notations as it focuses on the physical (perceptual) properties of notations rather than their logical (semantic) properties. The principles were synthesized from theory and empirical evidence from a wide range of fields and rest on an explicit theory of how visual notations communicate. They can be used to evaluate, compare, and improve existing visual notations as well as to construct new ones. The paper identifies serious design flaws in some of the leading SE notations, together with practical suggestions for improving them. It also showcases some examples of visual notation design excellence from SE and other fields.},
author = {Moody, Daniel},
doi = {10.1109/TSE.2009.67},
isbn = {978-1-60558-719-6},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Analysis,Communication,Computer industry,Concrete,Concrete syntax,Design optimization,Diagrams,Flowcharts,Humans,Modeling,Physics,Problem-solving,Software engineering,Unified modeling language,Visual syntax,Visualization,analysis,communication,concrete syntax.,design flaws,diagrams,physics of notations,visual notations,visual representation,visual syntax},
mendeley-tags = {Computer industry,Concrete,Design optimization,Flowcharts,Humans,Modeling,Physics,Problem-solving,Software engineering,Unified modeling language,Visualization,analysis,communication,concrete syntax.,design flaws,diagrams,physics of notations,visual notations,visual representation,visual syntax},
month = {nov},
number = {6},
pages = {756--779},
shorttitle = {The {\#}x0201C;Physics {\#}x0201D; of Notations},
title = {{The physics of notations: Toward a scientific basis for constructing visual notations in software engineering}},
volume = {35},
year = {2009}
}
@article{Sam2018,
author = {Sam, Stella and Hitzler, Pascal and Janowicz, Krzysztof},
doi = {10.3233/SW-180290},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sam, Hitzler, Janowicz - 2018 - On the Quality of Vocabularies for Linked Dataset Papers Published in the Semantic Web Journal.pdf:pdf},
issn = {22104968},
journal = {Semantic Web},
pages = {1--0},
title = {{On the Quality of Vocabularies for Linked Dataset Papers Published in the Semantic Web Journal}},
url = {http://semantic-web-journal.net/system/files/swj1794.pdf},
volume = {0},
year = {2018}
}
@article{Baum2017,
author = {Baum, Benjamin and Bauer, Christian R. and Thomas, Franke and Harald, Kusch and Marcel, Parciak and Thorsten, Rottmann and Nadine, Umbach and Ulrich, Sax and Benjamin, Baum and Bauer, Christian R. and Thomas, Franke and Harald, Kusch and Marcel, Parciak and Thorsten, Rottmann and Nadine, Umbach and Ulrich, Sax},
doi = {10.1515/itit-2016-0031},
isbn = {16112776},
journal = {it - Information Technology},
number = {4},
pages = {191--196},
title = {{Opinion paper: Data provenance challenges in biomedical research}},
url = {https://www.degruyter.com/view/j/itit.2017.59.issue-4/itit-2016-0031/itit-2016-0031.xml},
volume = {59},
year = {2017}
}
@article{Jiang2018,
abstract = {Geospatial data provenance is a fundamental issue in distributing spatial information on the Web. In the geoinformatics domain, provenance is often referred to as lineage. While the ISO 19115 lineage model is used widely in spatial data infrastructures, W3C has recommended the W3C provenance (PROV) data model for capturing and sharing provenance information on the Web. The use of these two separate efforts needs to be harmonized so that geospatial information does not remain an isolated area on the Web. Motivated by several domain use cases, we synthesize a list of provenance questions and analyze gaps between the two models in addressing these questions. Our strategy is to enrich W3C PROV with domain semantics from the ISO 19115 lineage model by suggesting ways to bridge them. A semantic mapping between the ISO lineage model and the W3C PROV model is formalized, and key issues involved are discussed. Use cases illustrate the applicability of the approach.},
author = {Jiang, Liangcun and Yue, Peng and Kuhn, Werner and Zhang, Chenxiao and Yu, Changhui and Guo, Xia},
doi = {10.1016/j.cageo.2018.05.001},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2018 - Advancing interoperability of geospatial data provenance on the web Gap analysis and strategies.pdf:pdf},
issn = {00983004},
journal = {Computers and Geosciences},
keywords = {Data lineage,Geospatial data provenance,ISO 19115,Interoperability,Lineage model,Prov,Provenance,Spatial,W3C PROV},
mendeley-tags = {Data lineage,Interoperability,Prov,Provenance,Spatial},
number = {March},
pages = {21--31},
publisher = {Elsevier Ltd},
title = {{Advancing interoperability of geospatial data provenance on the web: Gap analysis and strategies}},
url = {https://doi.org/10.1016/j.cageo.2018.05.001},
volume = {117},
year = {2018}
}
@article{Jankun-Kelly2007,
abstract = {Visualization exploration is the process of extracting insight from data via interaction with visual depictions of that data. Visualization exploration is more than presentation; the interaction with both the data and its depiction is as important as the data and depiction itself. Significant visualization research has focused on the generation of visualizations (the depiction); less effort has focused on the exploratory aspects of visualization (the process). However, without formal models of the process, visualization exploration sessions cannot be fully utilized to assist users and system designers. Toward this end, we introduce the P-Set Model of Visualization Exploration for describing this process and a framework to encapsulate, share, and analyze visual explorations. In addition, systems utilizing the model and framework are more efficient as redundant exploration is avoided. Several examples drawn from visualization applications demonstrate these benefits. Taken together, the model and framework provide an effective means to exploit the information within the visual exploration process.},
author = {Jankun-Kelly, T. J. and Ma, Kwan-Liu and Gertz, Michael},
doi = {10.1109/TVCG.2007.28},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jankun-Kelly, Ma, Gertz - 2007 - A Model and Framework for Visualization Exploration.pdf:pdf},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Index Terms—Visualization exploration process,XML,collaboration,derivation,history,science of visualization,software framework,visualization,visualization systems},
number = {2},
pages = {357--369},
title = {{A Model and Framework for Visualization Exploration}},
url = {https://ieeexplore.ieee.org/ielx5/2945/4069227/04069243.pdf?tp={\&}arnumber=4069243{\&}isnumber=4069227},
volume = {13},
year = {2007}
}
@article{Stamatogiannakis2017,
abstract = {Information produced by Internet applications is inherently a result of processes that are executed locally. Think of a web server that makes use of a CGI script, or a content management system where a post was first edited using a word processor. Given the impact of these processes to the content published online, a consumer of that information may want to understand what those impacts were. For example, understanding from where text was copied and pasted to make a post, or if the CGI script was updated with the latest security patches, may all influence the confidence on the published content. Capturing and exposing this information provenance is thus important in order to ascertaining trust to online content. Furthermore, providers of internet applications may wish to have access to the same information for debugging or audit purposes. For processes following a rigid structure (such as databases or workflows), disclosed provenance systems have been developed that efficiently and accurately capture the provenance of the produced data. However, accurately capturing provenance from unstructured processes, e.g. user-interactive computing used to produce web content, remains a problem to be tackled. In this paper, we address the problem of capturing and exposing provenance from unstructured processes. Our approach, called PROV 2R (PROVenance Record and Replay) is composed of two parts: 1) the decoupling of provenance analysis from its capture; and 2) the capture of high fidelity provenance from unmodified programs. We use techniques originating in the security and reverse engineering communities, namely, record and replay and taint tracking. Taint tracking fundamentally addresses the data provenance problem, but is impractical to apply at runtime due to extremely high overhead. With a number of case studies we demonstrate that PROV 2R enables the use of taint analysis for high fidelity provenance capture, while keeping the runtime overhead at manageable levels. In addition, we show how captured information can be represented using the W3C PROV provenance model for exposure on the Web.},
author = {Stamatogiannakis, Manolis and Athanasopoulos, Elias and Bos, Herbert and Groth, Paul},
doi = {10.1145/3062176},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stamatogiannakis et al. - 2017 - PROV2R Practical Provenance Analysis of Unstructured Processes.pdf:pdf},
issn = {15576051},
journal = {ACM Transactions on Internet Technology (TOIT)},
keywords = {Automation,Decoupling,Human in the loop,Interactive,Manual,Prov,Provenance,Strucutred,Unstructured,Workflows},
mendeley-tags = {Automation,Decoupling,Human in the loop,Interactive,Manual,Prov,Provenance,Strucutred,Unstructured,Workflows},
number = {4},
title = {{PROV2R: Practical Provenance Analysis of Unstructured Processes}},
volume = {17},
year = {2017}
}
@article{Shadbolt2006,
abstract = {The original Scientific American article on the Semantic Web appeared in 2001. It described the evolution of a Web that consisted largely of documents for humans to read to one that included data and information for computers to manipulate. The Semantic Web is a Web of actionable information--information derived from data through a semantic theory for interpreting the symbols.This simple idea, however, remains largely unrealized. Shopbots and auction bots abound on the Web, but these are essentially handcrafted for particular tasks; they have little ability to interact with heterogeneous data and information types. Because we haven't yet delivered large-scale, agent-based mediation, some commentators argue that the Semantic Web has failed to deliver. We argue that agents can only flourish when standards are well established and that the Web standards for expressing shared meaning have progressed steadily over the past five years. Furthermore, we see the use of ontologies in the e-science community presaging ultimate success for the Semantic Web--just as the use of HTTP within the CERN particle physics community led to the revolutionary success of the original Web. This article is part of a special issue on the Future of AI.},
author = {Shadbolt, Nigel and Hall, Wendy and Berners-Lee, Tim},
doi = {10.1109/MIS.2006.62},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shadbolt, Hall, Berners-Lee - 2006 - The semantic web revisited.pdf:pdf},
issn = {15411672},
journal = {IEEE Intelligent Systems},
number = {3},
pages = {96--101},
title = {{The semantic web revisited}},
volume = {21},
year = {2006}
}
@book{Moreau2013,
abstract = {?eWorldWideWeb is now deeply intertwined with our lives, and has become a catalyst for a data deluge, making vast amounts of data available online, at a click of a button.WithWeb 2.0, users are no longer passive consumers, but active publishers and curators of data. Hence, from science to food manufacturing, from data journalism to personal well-being, from social media to art, there is a strong interest in provenance, a description of what influenced an artifact, a data set, a document, a blog, or any resource on theWeb and beyond. Provenance is a crucial piece of information that can help a consumer make a judgment as to whether something can be trusted. Provenance is no longer seen as a curiosity in art circles, but it is regarded as pragmatically, eth- ically, and methodologically crucial for our day-to-day data manipulation and curation activities on theWeb. Following the recent publication of the prov standard for provenance on theWeb, which the two authors actively help shape in the ProvenanceWorking Group at theWorldWideWeb Consortium, this Synthesis lecture is a hands-on introduction to prov aimed atWeb and linked data professionals. By means of recipes, illustrations, a website at www.provbook.org, and tools, it guides practitioners through a variety of issues related to provenance: how to generate prove- nance, publish it on the Web, make it discoverable, and how to utilize it. Equipped with this knowledge, practictioners will be in a position to develop novel applications that can bring open- ness, trust, and accountability. KEYWORDS},
author = {Moreau, Luc and Groth, Paul},
booktitle = {Synthesis Lectures on the Semantic Web: Theory and Technology},
doi = {10.2200/S00528ED1V01Y201308WBE007},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moreau, Groth - 2013 - Provenance An Introduction to PROV.pdf:pdf},
isbn = {9781627052214},
keywords = {audit,audit trail,compliance,prov,provenance,semantic web,traceability},
title = {{Provenance: An Introduction to PROV}},
year = {2013}
}
@article{Compton2014,
abstract = {This paper presents an alignment between the W3C ProvenanceWorking Group's recommended ontology (PROV-O) and the W3C Semantic Sensor Networks Incubator Group's ontology (SSNO). The alignment views PROV-O as an upper ontology which is extended with SSNO concepts and properties. This allows representation of observation details and sensor deployments that are not possible in the SSNO alone, and gives a basis for alignment with Open Geospatial Consortium Observations {\&} Measurements aligned ontologies. Further to the alignment, rules are presented that further constrain the interpretation of the aligned ontologies and provide a mechanism by which provenance information can be generated from SSN data thereby allowing modellers to take advantage of the new features. The bene t of the aligned ontologies is illustrated with an example of cross-domain provenance querying enabled by the alignment.},
author = {Compton, Michael and Corsar, David and Taylor, Kerry},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Compton, Corsar, Taylor - 2014 - Sensor data provenance SSNO and PROV-O together at last.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
pages = {67--82},
title = {{Sensor data provenance: SSNO and PROV-O together at last}},
volume = {1401},
year = {2014}
}
@article{Gil2007,
abstract = {Workflows have emerged as a paradigm for representing and managing complex distributed computations and are used to accelerate the pace of scientific progress. A recent National Science Foundation workshop brought together domain, computer, and social scientists to discuss requirements of future scientific applications and the challenges they present to current workflow technologies.},
author = {Gil, Yolanda and Deelman, Ewa and Ellisman, Mark and Fahringer, Thomas and Fox, Geoffrey and Gannon, Dennis and Goble, Carole and Livny, Miron and Moreau, Luc and Myers, Jim},
doi = {10.1109/MC.2007.421},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gil et al. - 2007 - Examining the challenges of scientific workflows.pdf:pdf},
isbn = {0018-9162 VO - 40},
issn = {00189162},
journal = {Computer},
keywords = {Collaboratories,Computing practices,Scientific workflows},
number = {12},
pages = {24--32},
pmid = {21072299},
title = {{Examining the challenges of scientific workflows}},
volume = {40},
year = {2007}
}
@inproceedings{Peroni2017,
abstract = {Vector space embeddings have been shown to perform well when using RDF data in data mining and machine learning tasks. Exist-ing approaches, such as RDF2Vec, use local information, i.e., they rely on local sequences generated for nodes in the RDF graph. For word embeddings, global techniques, such as GloVe, have been proposed as an alternative. In this paper, we show how the idea of global embeddings can be transferred to RDF embeddings, and show that the results are competitive with traditional local techniques like RDF2Vec.},
author = {Peroni, Silvio and Shotton, David and Vitali, Fabio},
booktitle = {International Semantic Web Conference},
doi = {10.1007/978-3-319-68204-4_19},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peroni, Shotton, Vitali - 2017 - One Year of the OpenCitations Corpus.pdf:pdf},
isbn = {978-3-319-68203-7},
keywords = {Accademia,Citations,Prov,corpus,occ,open citation data,opencitations,public domain,spar ontologies},
mendeley-tags = {Accademia,Citations,Prov},
pages = {184--192},
publisher = {Springer},
title = {{One Year of the OpenCitations Corpus}},
url = {http://link.springer.com/10.1007/978-3-319-68204-4{\_}19},
year = {2017}
}
@article{Ziemann2016,
abstract = {The spreadsheet software Microsoft Excel, when used with default settings, is known to convert gene names to dates and floating-point numbers. A programmatic scan of leading genomics journals reveals that approximately one-fifth of papers with supplementary Excel gene lists contain erroneous gene name conversions.$\backslash$r$\backslash$n$\backslash$r$\backslash$n$\backslash$r$\backslash$nNew comment! Gene name errors are widespread in the scientific literature: Microsoft Excel ruining gene names strikes again, affecting {\textgreater}700 papers.$\backslash$r$\backslash$n},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Ziemann, Mark and Eren, Yotam and El-Osta, Assam},
doi = {10.1186/s13059-016-1044-7},
eprint = {1011.1669},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ziemann, Eren, El-Osta - 2016 - Gene name errors are widespread in the scientific literature.pdf:pdf},
isbn = {1807-5932},
issn = {1474760X},
journal = {Genome Biology},
keywords = {Gene symbol,Microsoft Excel,Supplementary data},
number = {1},
pages = {17--19},
pmid = {27552985},
publisher = {Genome Biology},
title = {{Gene name errors are widespread in the scientific literature}},
url = {http://dx.doi.org/10.1186/s13059-016-1044-7},
volume = {17},
year = {2016}
}
@article{Missier2013,
abstract = {This paper presents an extension to the {\{}W3C{\}} {\{}PROV{\}} provenance model, aimed at representing process structure. Although the modelling of process structure is out of the scope of the {\{}PROV{\}} specification, it is beneficial when capturing and analyzing the provenance of data that is produced by programs or other formally encoded processes. In the paper, we motivate the need for such and extended model in the context of an ongoing large data federation and preservation project, {\{}DataONE2{\}}, where provenance traces of scientific workflow runs are captured and stored alongside the data products. We introduce new provenance relations for modelling process structure along with their usage patterns, and present sample queries that demonstrate their benefit.},
author = {Missier, Paolo and Dey, Saumen and Belhajjame, Khalid and Cuevas-Vicentt{\'{i}}n, V{\'{i}}ctor and Lud{\"{a}}scher, Bertram},
doi = {10.1145/2457317.2457375},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Missier et al. - 2013 - D-PROV Extending the PROV Provenance Model with Workflow Structure.pdf:pdf},
isbn = {9781450315999},
journal = {Proceedings of the 5th USENIX Workshop on the Theory and Practice of Provenance},
keywords = {provenance,workflow},
mendeley-tags = {provenance,workflow},
pages = {9:1--9:7},
title = {{D-PROV: Extending the PROV Provenance Model with Workflow Structure}},
url = {http://dl.acm.org/citation.cfm?id=2482949.2482961},
year = {2013}
}
@article{Massi2018,
abstract = {Provenance is the foundation of data quality, usually implemented by automatically capturing the trace of data manipulation over space and time. In healthcare, provenance becomes critical since it encompasses both clinical research and patient safety. In this proposal we aim at exploiting and innovating existing health IT deployments by enabling data provenance queries for all kind of clinical information from anywhere. The proposed technical solution exploits the novelty and the peer-to-peer fashion of the blockchain technology and smart-contracts to instrument international standards such as IHE and HL7 with a provenance system robust to fraudulences.},
author = {Massi, Massimiliano and Miladi, Abdallah and Margheri, Andrea and Sassone, Vladimiro and Rosenzweig, Jason},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Massi et al. - 2018 - Using PROV and Blockchain to Achieve Health Data Provenance.pdf:pdf},
keywords = {Blockchain,Provenance},
mendeley-tags = {Blockchain,Provenance},
pages = {1--24},
title = {{Using PROV and Blockchain to Achieve Health Data Provenance}},
url = {https://eprints.soton.ac.uk/421292/},
year = {2018}
}
@article{Buneman2001,
abstract = {With the proliferation of database views and curated databases, the issue of data provenance - where a piece of data came from and the process by which it arrived in the database - is becoming increasingly important, especially in scientific databases where understanding provenance is crucial to the accuracy and currency of data. In this paper we describe an approach to computing provenance when the data of interest has been created by a database query. We adopt a syntactic approach and present results for a general data model that applies to relational databases as well as to hierarchical data such as XML. A novel aspect of our work is a distinction between “why” provenance (refers to the source data that had some influence on the existence of the data) and “where” provenance (refers to the location(s) in the source databases from which the data was extracted).},
author = {Buneman, Peter and Khanna, Sanjeev and Tan, W and Wang-Chiew, Tan},
doi = {10.1007/3-540-44503-X_20},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buneman et al. - 2001 - Why and Where A Characterization of Data Provenance.pdf:pdf},
isbn = {9783540414568},
issn = {02698463},
journal = {Proceedings of International Conference on Database Theory (ICDT)},
pages = {316--330},
pmid = {7011450},
title = {{Why and Where: A Characterization of Data Provenance}},
volume = {1973},
year = {2001}
}
@inproceedings{Bachour2015,
author = {Bachour, Khaled and Wetzel, Richard and Flintham, Martin and Huynh, Trung Dong and Rodden, Tom A. and Moreau, Luc},
booktitle = {33rd Annual ACM Conference on Human Factors in Computing Systems (CHI'15)},
doi = {10.1145/2702123.2702455},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bachour et al. - 2015 - Provenance for the People An HCI Perspective on the W3C PROV Standard through an Online Game.pdf:pdf},
keywords = {HCI,Prov,Usability},
mendeley-tags = {HCI,Prov,Usability},
title = {{Provenance for the People: An HCI Perspective on the W3C PROV Standard through an Online Game}},
url = {http://eprints.nottingham.ac.uk/37612/1/PROV CHI v1.4 camera ready.pdf},
year = {2015}
}
@article{Oinn2004,
abstract = {MOTIVATION: In silico experiments in bioinformatics involve the co-ordinated use of computational tools and information repositories. A growing number of these resources are being made available with programmatic access in the form of Web services. Bioinformatics scientists will need to orchestrate these Web services in workflows as part of their analyses. RESULTS: The Taverna project has developed a tool for the composition and enactment of bioinformatics workflows for the life sciences community. The tool includes a workbench application which provides a graphical user interface for the composition of workflows. These workflows are written in a new language called the simple conceptual unified flow language (Scufl), where by each step within a workflow represents one atomic task. Two examples are used to illustrate the ease by which in silico experiments can be represented as Scufl workflows using the workbench application.},
author = {Oinn, Tom and Addis, Matthew and Ferris, Justin and Marvin, Darren and Senger, Martin and Greenwood, Mark and Carver, Tim and Glover, Kevin and Pocock, Matthew R. and Wipat, Anil and Li, Peter},
doi = {10.1093/bioinformatics/bth361},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oinn et al. - 2004 - Taverna A tool for the composition and enactment of bioinformatics workflows.pdf:pdf},
isbn = {1367-4803 (Print)},
issn = {13674803},
journal = {Bioinformatics},
month = {nov},
number = {17},
pages = {3045--3054},
pmid = {15201187},
shorttitle = {Taverna},
title = {{Taverna: A tool for the composition and enactment of bioinformatics workflows}},
url = {https://academic.oup.com/bioinformatics/article/20/17/3045/186405/Taverna-a-tool-for-the-composition-and-enactment},
volume = {20},
year = {2004}
}
@article{Tan2007,
author = {Tan, Wang-Chiew},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan - 2007 - Provenance in Databases Past, Current, and Future.pdf:pdf},
journal = {IEEE Data Engineering Bulletin},
keywords = {Databases,Provenance},
mendeley-tags = {Databases,Provenance},
number = {4},
pages = {3--12},
title = {{Provenance in Databases: Past, Current, and Future}},
url = {http://sites.computer.org/debull/A07dec/wang-chiew.pdf},
volume = {30},
year = {2007}
}
@article{Bowers2006,
abstract = {Integrated provenance support promises to be a chief advantage of scientific workflow systems over script-based alternatives. While it is often recognized that information gathered during scientific workflow execution can be used automatically to increase fault tolerance (via checkpointing) and to optimize performance (by reusing intermediate data products in future runs), it is perhaps more significant that provenance information may also be used by scientists to reproduce results from earlier runs, to explain unexpected results, and to prepare results for publication. Current workflow systems offer little or no direct support for these "scientist-oriented" queries of provenance information. Indeed the use of advanced execution models in scientific workflows (e.g., process networks, which exhibit pipeline parallelism over streaming data) and failure to record certain fundamental events such as state resets of processes, can render existing provenance schemas useless for scientific applications of provenance. We develop a simple provenance model that is capable of supporting a wide range of scientific use cases even for complex models of computation such as process networks. Our approach reduces these use cases to database queries over event logs, and is capable of reconstructing complete data and invocation dependency graphs for a workflow run. Abstract. Integrated provenance support promises to be a chief advan-tage of scientific workflow systems over script-based alternatives. While it is often recognized that information gathered during scientific work-flow execution can be used automatically to increase fault tolerance (via checkpointing) and to optimize performance (by reusing intermediate data products in future runs), it is perhaps more significant that prove-nance information may also be used by scientists to reproduce results from earlier runs, to explain unexpected results, and to prepare results for publication. Current workflow systems offer little or no direct support for these " scientist-oriented " queries of provenance information. Indeed the use of advanced execution models in scientific workflows (e.g., pro-cess networks, which exhibit pipeline parallelism over streaming data) and failure to record certain fundamental events such as state resets of processes, can render existing provenance schemas useless for scientific applications of provenance. We develop a simple provenance model that is capable of supporting a wide range of scientific use cases even for complex models of computation such as process networks. Our approach reduces these use cases to database queries over event logs, and is capa-ble of reconstructing complete data and invocation dependency graphs for a workflow run.},
author = {Bowers, Shawn and Mcphillips, Timothy and Ludascher, Bertram and Cohen, Shirley and Davidson, Susan B and Lud{\"{a}}scher, Bertram},
doi = {10.1007/11890850_15},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bowers et al. - 2006 - A Model for User-Oriented Data Provenance in Pipelined Scientific Workflows.pdf:pdf},
isbn = {9783540463023},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
keywords = {Kepler,bioinformatics,databases,workflows},
number = {4145},
pages = {133--147},
pmid = {9199103},
title = {{A Model for User-Oriented Data Provenance in Pipelined Scientific Workflows}},
url = {http://dx.doi.org/10.1007/11890850{\_}15{\%}5Cnhttp://repository.upenn.edu/cis{\_}papers/290},
volume = {4145},
year = {2006}
}
@article{Mates2011,
abstract = {Managing and understanding the growing volumes of scientific data is one of the most challenging issues scientists face today. As analyses get more complex and large interdisciplinary groups need to work together, knowledge sharing becomes essential to support effective scientific data exploration. While science portals and visualization Web sites have provided a first step towards this goal, by aggregating data from different sources and providing a set of predesigned analyses and visualizations, they have important limitations. Often, these sites are built manually and are not flexible enough to support the vast heterogeneity of data sources, analysis techniques, data products, and the needs of different user communities. In this paper we describe CrowdLabs, a system that adopts the model used by social Web sites, allowing users to share not only data but also computational pipelines. The shared repository opens up many new opportunities for knowledge sharing and re-use, exposing scientists to tasks that provide examples of sophisticated uses of algorithms they would not have access to otherwise. CrowdLabs combines a set of usable tools and a scalable infrastructure to provide a rich collaborative environment for scientists, taking into account the requirements of computational scientists, such as accessing high-performance computers and manipulating large amounts of data.},
author = {Mates, Phillip and Santos, Emanuele and Freire, Juliana and Silva, Cl{\'{a}}udio T.},
doi = {10.1007/978-3-642-22351-8_38},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mates et al. - 2011 - CrowdLabs Social analysis and visualization for the sciences.pdf:pdf},
isbn = {9783642223501},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Computational Sciences,Cyberinfrastructure,Visualization},
pages = {555--564},
title = {{CrowdLabs: Social analysis and visualization for the sciences}},
volume = {6809 LNCS},
year = {2011}
}
@article{Nielsen1994,
abstract = {Keywords},
author = {Nielsen, Jakob},
doi = {10.1145/259963.260333},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nielsen - 1994 - Enhancing the explanatory power of usability heuristics.pdf:pdf},
isbn = {0897916514},
journal = {Conference companion on Human factors in computing systems - CHI '94},
pages = {210},
pmid = {1157471},
title = {{Enhancing the explanatory power of usability heuristics}},
url = {http://portal.acm.org/citation.cfm?doid=259963.260333},
year = {1994}
}
@article{Cox2015,
abstract = {The PROV data model is becoming accepted as a flexible and robust tool for formalizing information relating to the production of documents and datasets. Provenance stores based on the PROV-O implementation are appearing in support of scientific data workflows. However, the scope of PROV does not have to be limited to digital or information assets. For example, specimens typically undergo complex preparation sequences prior to actual observations and measurements, and it is important to record this to ensure reproducibility and to enable assessment of the reliability of data produced. PROV provides a flexible solution, allowing a comprehensive trace of predecessor entities and transformations at any level of detail. In this paper we demonstrate the use of PROV for describing specimens managed for scientific observations. Two examples are considered: a geological sample which undergoes a typical preparation process for measurements of the concentration of a particular chemical substance, and the collection, taxonomic classification and eventual publication of an insect specimen. We briefly compare PROV with related work. 1. INTRODUCTION The concept of provenance was developed in the art, museums and archives community, referring to a record of the history of an artefact. Tracing the history of ownership and custodianship is an important means of determining the authenticity of rare or unique items. More recently, the same term has been applied to the lineage of information objects -particularly datasets, imagery, etc. These may have a complex history with multiple data-processing steps, the details of which are important in evaluating the quality or fitness for a particular purpose, and also for establishing reproducibility which is a core tenet of empirical science. In this setting, a 'provenance trace' can be seen as the record of an instantiated 'workflow'. The W3C PROV model [6,9] harmonizes a number of earlier treatments (in particular PML and OPM) and is becoming accepted as the basis for formalizing information relating to the production of documents and datasets. Provenance stores based on the PROV-O implementation, such as PROMS [1], are appearing in support of scientific data workflows. In the context of technical and scientific collections, where specimens (biological, water, soil and rock) are managed to support subsequent observations, chain of custody concerns do arise, particularly in relation to forensic applications, and also where there are financial implications from results of observations on samples, such as assays on mineral exploration specimens. But the key feature of technical and scientific specimens is the preparation process, generating a sequence of samples -some of which only exist temporarily -which are related through various processing activities, in support of the observational requirements. There is enormous variety in the process-chains related to these, both between and within disciplines. In fact, the design of new sequences is a key activity in empirical science. The PROV ontology -which abstracts all possible processing chains into a single high level model whereby the production and transformation of Entities is through time-bounded Activities, under the influence or control of Agents -appears to provide a framework that can satisfy all of the relevant concerns, either directly or through minor specializations within a basic framework.},
author = {Cox, S. J. D. and Car, N. J.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cox, Car - 2015 - PROV and Real Things.pdf:pdf},
journal = {MODSIM2015, 21st International Congress on Modelling and Simulation},
keywords = {PROV,Physical,Provenance,sampling,specimen},
mendeley-tags = {Physical,Provenance},
number = {November},
pages = {620--626},
title = {{PROV and Real Things}},
url = {http://mssanz.org.au/modsim2015/C4/cox.pdf},
year = {2015}
}
@inproceedings{Zhao2012,
author = {Zhao, Jun and Gomez-Perez, Jose Manuel and Belhajjame, Khalid and Klyne, Graham and Garcia-Cuesta, Esteban and Garrido, Aleix and Hettne, Kristina and Roos, Marco and {De Roure}, David and Goble, Carole},
booktitle = {2012 IEEE 8th International Conference on E-Science},
doi = {10.1109/eScience.2012.6404482},
isbn = {978-1-4673-4466-1},
keywords = {practice,taverna,workflows},
mendeley-tags = {practice,taverna,workflows},
month = {oct},
pages = {1--9},
publisher = {IEEE},
title = {{Why workflows break - Understanding and combating decay in Taverna workflows}},
url = {http://ieeexplore.ieee.org/document/6404482/},
year = {2012}
}
@article{Moreau2015,
abstract = {The prov family of documents are the final output of the World Wide Web Consortium Provenance Working Group, chartered to specify a representation of provenance to facilitate its exchange over the Web. This article reflects upon the key requirements, guiding principles, and design decisions that influenced the prov family of documents. A broad range of requirements were found, relating to the key concepts necessary for describing provenance, such as resources, activities, agents and events, and to balancing prov's ease of use with the facility to check its validity. By this retrospective requirement analysis, the article aims to provide some insights into how prov turned out as it did and why. Benefits of this insight include better inter-operability, a roadmap for alternate investigations and improvements, and solid foundations for future standardization activities.},
author = {Moreau, Luc and Groth, Paul and Cheney, James and Lebo, Timothy and Miles, Simon},
doi = {10.1016/j.websem.2015.04.001},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moreau et al. - 2015 - The rationale of PROV.pdf:pdf},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Design decision,PROV,Provenance,Rationale,Requirement,Standardization,prov},
mendeley-tags = {PROV,Provenance},
pages = {235--257},
publisher = {Elsevier B.V.},
title = {{The rationale of PROV}},
url = {http://dx.doi.org/10.1016/j.websem.2015.04.001},
volume = {35},
year = {2015}
}
@article{Cheney2011,
abstract = {Provenance is information recording the source, derivation, or history of some information. Provenance tracking has been studied in a variety of settings; however, although many design points have been explored, the mathematical or semantic foundations of data provenance have received comparatively little attention. In this paper, we argue that dependency analysis techniques familiar from program analysis and program slicing provide a formal foundation for forms of provenance that are intended to show how (part of) the output of a query depends on (parts of) its input. We introduce a semantic characterization of such dependency provenance, show that this form of provenance is not computable, and provide dynamic and static approximation techniques.},
archivePrefix = {arXiv},
arxivId = {arXiv:0708.2173v2},
author = {Cheney, James and Ahmed, Amal and Acar, Umut A.},
doi = {10.1017/S0960129511000211},
eprint = {arXiv:0708.2173v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheney, Ahmed, Acar - 2011 - Provenance as dependency analysis.pdf:pdf},
isbn = {978-3-540-75987-4},
issn = {09601295},
journal = {Mathematical Structures in Computer Science},
number = {6},
pages = {1301--1337},
title = {{Provenance as dependency analysis}},
volume = {21},
year = {2011}
}
