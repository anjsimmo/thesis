Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Regalia2016,
abstract = {The 47 revised full papers presented together with three invited talks were carefully reviewed and selected from 204 submissions. This program was completed by a demonstration and poster session, in which researchers had the chance to present their latest results and advances in the form of live demos. In addition, the PhD Symposium program included 10 contributions, selected out of 21 submissions. The core tracks of the research conference were complemented with new tracks focusing on linked data; machine learning; mobile web, sensors and semantic streams; natural language processing and information retrieval; reasoning; semantic data management, big data, and scalability; services, APIs, processes and cloud computing; smart cities, urban and geospatial data; trust and privacy; and vocabularies, schemas, and ontologies.},
author = {Regalia, Blake and Janowicz, Krzysztof and Gao, Song},
doi = {10.1007/978-3-319-34129-3_32},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Regalia, Janowicz, Gao - 2016 - VOLT A Provenance-Producing, Transparent SPARQL Proxy for the On-Demand Computation of Linked Data and i.pdf:pdf},
isbn = {978-3-319-34128-6},
journal = {The Semantic Web. Latest Advances and New Domains},
keywords = {cyber-infrastructure,geospatial semantics,linked data,semantic web},
pages = {523--538},
title = {{VOLT: A Provenance-Producing, Transparent SPARQL Proxy for the On-Demand Computation of Linked Data and its Application to Spatiotemporally Dependent Data}},
url = {http://link.springer.com/10.1007/978-3-319-34129-3},
volume = {9678},
year = {2016}
}
@article{Simmhan2005,
abstract = {Data management is growing in complexity as large-scale applications take advantage of the loosely coupled resources brought together by grid middleware and by abundant storage capacity. Metadata describing the data products used in and generated by these applications is essential to disambiguate the data and enable reuse. Data provenance, one kind of metadata, pertains to the derivation history of a data product starting from its original sources.In this paper we create a taxonomy of data provenance characteristics and apply it to current research efforts in e-science, focusing primarily on scientific workflow approaches. The main aspect of our taxonomy categorizes provenance systems based on why they record provenance, what they describe, how they represent and store provenance, and ways to disseminate it. The survey culminates with an identification of open research problems in the field.},
author = {Simmhan, Yogesh L. and Plale, Beth and Gannon, Dennis},
doi = {10.1145/1084805.1084812},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simmhan, Plale, Gannon - 2005 - A Survey of Data Provenance in e-Science.pdf:pdf},
issn = {0163-5808},
journal = {ACM SIGMOD Record},
number = {3},
pages = {31--36},
title = {{A Survey of Data Provenance in e-Science}},
url = {http://doi.acm.org/10.1145/1084805.1084812{\%}5Cnhttp://dl.acm.org/ft{\_}gateway.cfm?id=1084812{\&}type=pdf},
volume = {34},
year = {2005}
}
@article{Callahan2006,
abstract = {Scientists are now faced with an incredible volume of data to analyze. To successfully analyze and validate various hypothesis, it is necessary to pose several queries, correlate disparate data, and create insightful visualizations of both the simulated processes and observed phenomena. Often, insight comes from comparing the results of multiple visualizations. Unfortunately, today this process is far from interactive and contains many error-prone and time-consuming tasks. As a result, the generation and maintenance of visualizations is a major bottleneck in the scientific process, hindering both the ability to mine scientific data and the actual use of the data. The VisTrails system represents our initial attempt to improve the scientific discovery process and reduce the time to insight. In VisTrails, we address the problem of visualization from a data management perspective: VisTrails manages the data and metadata of a visualization product. In this demonstration, we show the power and flexibility of our system by presenting actual scenarios in which scientific visualization is used and showing how our system improves usability, enables reproducibility, and greatly reduces the time required to create scientific visualizations.},
author = {Callahan, Steven P. and Freire, Juliana and Santos, Emanuele and Scheidegger, Carlos E. and Silva, Cl{\'{a}}udio T. and Vo, Huy T.},
doi = {10.1145/1142473.1142574},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Callahan et al. - 2006 - VisTrails Visualization meets Data Management.pdf:pdf},
isbn = {1595934340},
issn = {07308078},
journal = {Proceedings of the 2006 ACM SIGMOD international conference on Management of data - SIGMOD '06},
pages = {745},
title = {{VisTrails: Visualization meets Data Management}},
url = {http://portal.acm.org/citation.cfm?doid=1142473.1142574},
year = {2006}
}
@inproceedings{Chapman2010,
abstract = {Provenance has been touted as a basis to establish trust in data. Intuitively, belief in a hypothesis should depend on how much one trusts the relevant data. However, current proposals to assess trust based solely on provenance are insufficient for rigourous decision making. We describe a model of provenance and belief that is necessary and sufficient to incorporate trust in the data in a way that supports normative inference. The model is based on the observation that provenance can be viewed as a causal structure which can be used to compute belief from assessments of the accuracy of sources and transformations that produced relevant data. In our model, data sources are like sensors with associated conditional probability tables. Provenance identifies dependencies among sensors. Together, this information allows construction of causal networks that can be used to compute the belief in a state of the world based on observation of data. This model formalizes the role of source accuracy, and provides a method for formally assessing belief that uses only information in the provenance store, not the contents of the data.},
address = {San Jose, California},
author = {Chapman, Adriane and Blaustein, Barbara and Elsaesser, Chris},
booktitle = {Proceedings of the 2nd Conference on Theory and Practice of Provenance},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chapman, Blaustein, Elsaesser - 2010 - Provenance-based Belief.pdf:pdf},
pages = {1--14},
publisher = {USENIX Association},
title = {{Provenance-based Belief}},
url = {https://dl.acm.org/citation.cfm?id=1855806},
year = {2010}
}
@article{Belhajjame2015,
abstract = {Abstract Scientific workflows are a popular mechanism for specifying and automating data-driven in silico experiments. A significant aspect of their value lies in their potential to be reused. Once shared, workflows become useful building blocks that can be combined or modified for developing new experiments. However, previous studies have shown that storing workflow specifications alone is not sufficient to ensure that they can be successfully reused, without being able to understand what the workflows aim to achieve or to re-enact them. To gain an understanding of the workflow, and how it may be used and repurposed for their needs, scientists require access to additional resources such as annotations describing the workflow, datasets used and produced by the workflow, and provenance traces recording workflow executions. In this article, we present a novel approach to the preservation of scientific workflows through the application of research objects - aggregations of data and metadata that enrich the workflow specifications. Our approach is realised as a suite of ontologies that support the creation of workflow-centric research objects. Their design was guided by requirements elicited from previous empirical analyses of workflow decay and repair. The ontologies developed make use of and extend existing well known ontologies, namely the Object Reuse and Exchange (ORE) vocabulary, the Annotation Ontology (AO) and the W3C PROV ontology (PROVO). We illustrate the application of the ontologies for building Workflow Research Objects with a case-study that investigates Huntington's disease, performed in collaboration with a team from the Leiden University Medial Centre (HG-LUMC). Finally we present a number of tools developed for creating and managing workflow-centric research objects.},
author = {Belhajjame, Khalid and Zhao, Jun and Garijo, Daniel and Gamble, Matthew and Hettne, Kristina and Palma, Raul and Mina, Eleni and Corcho, Oscar and G{\'{o}}mez-P{\'{e}}rez, Jos{\'{e}} Manuel and Bechhofer, Sean and Klyne, Graham and Goble, Carole},
doi = {10.1016/j.websem.2015.01.003},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Belhajjame et al. - 2015 - Using a suite of ontologies for preserving workflow-centric research objects.pdf:pdf},
isbn = {1570-8268},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Annotation,Ontologies,Preservation,Provenance,Research object,Scientific workflow},
pages = {16--42},
publisher = {Elsevier B.V.},
title = {{Using a suite of ontologies for preserving workflow-centric research objects}},
url = {http://dx.doi.org/10.1016/j.websem.2015.01.003},
volume = {32},
year = {2015}
}
@book{Widom2000,
abstract = {We consider the view data lineage problem in a warehousing en vironment. For a given data item in a materialized warehouse view, we want to identify the set of source data items that produced the view item. We formally define the lineage problem, develop lineage tracing algorithms for relational views with aggregation, and propose mechanisms for performing consistent lineage tracing in a multi-source data warehousing environment. Our results can form the basis of a tool that allows analysts to browse warehouse data, select view tuples of interest, then "drill-through" to examine the exact source tuples that produced the view tuples of interest.},
author = {Cui, Yingwei and Widom, Jennifer and Wiener, Janet L.},
booktitle = {ACM Transactions on Database Systems},
doi = {10.1145/357775.357777},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cui, Widom, Wiener - 2000 - Tracing the lineage of view data in a warehousing environment.pdf:pdf},
isbn = {3060296103},
issn = {03625915},
keywords = {database,lineage,provenance},
mendeley-tags = {database,lineage,provenance},
number = {2},
pages = {179--227},
title = {{Tracing the lineage of view data in a warehousing environment}},
url = {http://portal.acm.org/citation.cfm?doid=357775.357777},
volume = {25},
year = {2000}
}
@inproceedings{Yu2017,
author = {Yu, Xiao Liang and Xu, Xiwei and Liu, Bin},
booktitle = {International Conference on Advanced Information Systems Engineering},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu, Xu, Liu - 2017 - EthDrive A Peer-to-Peer Data Storage with Provenance.pdf:pdf},
keywords = {blockchain,indirect evaporative cooling,nicta,split-,storage,type air conditioner},
mendeley-tags = {blockchain,nicta,storage},
title = {{EthDrive: A Peer-to-Peer Data Storage with Provenance}},
url = {https://research.csiro.au/data61/wp-content/uploads/sites/85/2016/08/main.pdf},
year = {2017}
}
@article{Corrales2018,
abstract = {The data preprocessing is an essential step in knowledge discovery projects. The experts affirm that preprocessing tasks take between 50{\%} to 70{\%} of the total time of the knowledge discovery process. In this sense, several authors consider the data cleaning as one of the most cumbersome and critical tasks. Failure to provide high data quality in the preprocessing stage will significantly reduce the accuracy of any data analytic project. In this paper, we propose a framework to address the data quality issues in classification tasks DQF4CT. Our approach is composed of: (i) a conceptual framework to provide the user guidance on how to deal with data problems in classification tasks; and (ii) an ontology that represents the knowledge in data cleaning and suggests the proper data cleaning approaches. We presented two case studies through real datasets: physical activity monitoring (PAM) and occupancy detection of an office room (OD). With the aim of evaluating our proposal, the cleaned datasets by DQF4CT were used to train the same algorithms used in classification tasks by the authors of PAM and OD. Additionally, we evaluated DQF4CT through datasets of the Repository of Machine Learning Databases of the University of California, Irvine (UCI). In addition, 84{\%} of the results achieved by the models of the datasets cleaned by DQF4CT are better than the models of the datasets authors.},
author = {Corrales, David and Ledezma, Agapito and Corrales, Juan},
doi = {10.3390/sym10070248},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Corrales, Ledezma, Corrales - 2018 - From Theory to Practice A Data Quality Framework for Classification Tasks.pdf:pdf},
issn = {2073-8994},
journal = {Symmetry},
keywords = {classification task,conceptual framework,data cleaning ontology,data quality issue,dqf4ct},
number = {7},
pages = {248},
title = {{From Theory to Practice: A Data Quality Framework for Classification Tasks}},
url = {http://www.mdpi.com/2073-8994/10/7/248},
volume = {10},
year = {2018}
}
@article{Freire2008,
abstract = {The problem of systematically capturing and managing provenance for computational tasks has recently received significant attention because of its relevance to a wide range of domains and applications. The authors give an overview of important concepts related to provenance management, so that potential users can make informed decisions when selecting or designing a provenance solution.},
author = {Freire, Juliana and Koop, David and Santos, Emanuele and Silva, Cl{\'{a}}udio T.},
doi = {10.1109/MCSE.2008.79},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Freire et al. - 2008 - Provenance for computational tasks A survey.pdf:pdf},
isbn = {1521-9615},
issn = {15219615},
journal = {Computing in Science and Engineering},
keywords = {Data visualization,Provenance,VisTrails},
number = {3},
pages = {11--21},
title = {{Provenance for computational tasks: A survey}},
volume = {10},
year = {2008}
}
@article{Oliveira2017,
abstract = {Data provenance is a fundamental concept in scientific experimentation. However, for their proper understanding and use, efficient and user-friendly mechanisms are needed. Research in software visualization, ontologies and complex networks can help in this process. This paper presents a framework to assist in the understanding and use of data provenance using visualization techniques, ontologies and complex networks. The framework capture the provenance data and generates new information using ontologies and provenance graph analysis. The graph is analyzed through complex networks techniques and provide some metrics to help in each node analyzes. The visualization presents and highlights the inferences and results. The framework was used in the E-SECO scientific ecosystem to support the scientific experimentation.},
author = {Oliveira, Weiner and Ambr{\'{o}}sio, Lenitta M. and Braga, Regina and Str{\"{o}}ele, Victor and David, Jos{\'{e}} Maria and Campos, Fernanda},
doi = {10.1016/j.procs.2017.05.216},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oliveira et al. - 2017 - A Framework for Provenance Analysis and Visualization.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Complex Network,E-science,Provenance,Visualization,prov,provenance,usability,visualisation},
mendeley-tags = {prov,provenance,usability,visualisation},
pages = {1592--1601},
title = {{A Framework for Provenance Analysis and Visualization}},
volume = {108},
year = {2017}
}
@article{Brauer2012,
abstract = {The World Wide Web evolves into a Web of Data, a huge, globally distributed dataspace that contains a rich body of machine-processable information from a virtually unbound set of providers covering a wide range of topics. However, due to the openness of the Web little is known about who created the data and how. The fact that a large amount of the data on the Web is derived by replication, query processing, modification, or merging raises concerns of information quality. Poor quality data may propagate quickly and contaminate the Web of Data. Provenance information about who created and published the data and how, provides the means for quality assessment. This paper takes a first step towards creating a quality-aware Web of Data: we present approaches to integrate provenance information into the Web of Data and we illustrate how this information can be consumed. In particular, we introduce a vocabulary to describe provenance of Web data as metadata and we discuss possibilities to make such provenance metadata accessible as part of the Web of Data. Furthermore, we describe how this metadata can be queried and consumed to identify outdated information.},
archivePrefix = {arXiv},
arxivId = {1406.2495},
author = {Brauer, Peer C and Fittkau, Florian and B, Wilhelm Hasselbring},
doi = {10.1007/978-3-642-34222-6},
eprint = {1406.2495},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brauer, Fittkau, B - 2012 - Provenance and Annotation of Data and Processes.pdf:pdf},
isbn = {978-3-642-34221-9},
issn = {03029743},
pages = {223--225},
pmid = {9156564},
title = {{Provenance and Annotation of Data and Processes}},
url = {http://link.springer.com/10.1007/978-3-642-34222-6},
volume = {7525},
year = {2012}
}
@article{Cohen-Boulakia2017,
abstract = {With the development of new experimental technologies, biologists are faced with an avalanche of data to be computationally analyzed for scientific advancements and discoveries to emerge. Faced with the complexity of analysis pipelines, the large number of computational tools, and the enormous amount of data to manage, there is compelling evidence that many if not most scientific discoveries will not stand the test of time: increasing the reproducibility of computed results is of paramount importance. The objective we set out in this paper is to place scientific workflows in the context of reproducibility. To do so, we define several kinds of reproducibility that can be reached when scientific workflows are used to perform experiments. We characterize and define the criteria that need to be catered for by reproducibility-friendly scientific workflow systems, and use such criteria to place several representative and widely used workflow systems and companion tools within such a framework. We also discuss the remaining challenges posed by reproducible scientific workflows in the life sciences. Our study was guided by three use cases from the life science domain involving in silico experiments.},
author = {Cohen-Boulakia, Sarah and Belhajjame, Khalid and Collin, Olivier and Chopard, J{\'{e}}r{\^{o}}me and Froidevaux, Christine and Gaignard, Alban and Hinsen, Konrad and Larmande, Pierre and Bras, Yvan Le and Lemoine, Fr{\'{e}}d{\'{e}}ric and Mareuil, Fabien and M{\'{e}}nager, Herv{\'{e}} and Pradal, Christophe and Blanchet, Christophe},
doi = {10.1016/j.future.2017.01.012},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen-Boulakia et al. - 2017 - Scientific workflows for computational reproducibility in the life sciences Status, challenges and opport.pdf:pdf},
isbn = {0167-739X},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Packaging environments,Provenance,Reproducibility,Scientific workflows,case study,workflow},
mendeley-tags = {case study,workflow},
pages = {284--298},
publisher = {Elsevier B.V.},
title = {{Scientific workflows for computational reproducibility in the life sciences: Status, challenges and opportunities}},
url = {http://dx.doi.org/10.1016/j.future.2017.01.012},
volume = {75},
year = {2017}
}
@article{Buneman2006,
abstract = {Many curated databases are constructed by scientists integrating various existing data sources ``by hand'', that is, by manually entering or copying data from other sources. Capturing provenance in such an environment is a challenging problem, requiring a good model of the process of curation. Existing models of provenance focus on queries/views in databases or computations on the Grid, not updates of databases or Web sites. In this paper we motivate and present a simple model of provenance for manually curated databases and discuss ongoing and future work.},
author = {Buneman, Peter and Chapman, Adriane P. and Cheney, James},
doi = {10.1007/11890850_17},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buneman, Chapman, Cheney - 2006 - A Provenance Model for Manually Curated Data.pdf:pdf},
isbn = {3 540 46302 X},
issn = {03029743},
journal = {Provenance and {\ldots}},
pages = {162--170},
pmid = {9176211},
title = {{A Provenance Model for Manually Curated Data}},
url = {http://link.springer.com/chapter/10.1007/11890850{\_}17},
year = {2006}
}
@article{Acar2010,
abstract = {Provenance has been studied extensively in both database and workflow management systems, so far with little convergence of definitions or models. Provenance in databases has generally been defined for relational or complex object data, by propagating fine-grained annotations or algebraic expressions from the input to the output. This kind of provenance has been found useful in other areas of computer science: annotation databases, probabilistic databases, schema and data integration, etc. In contrast, workflow provenance aims to capture a complete description of evaluation - or enactment - of a workflow, and this is crucial to verification in scientific computation. Workflows and their provenance are often presented using graphical notation, making them easy to visualize but complicating the formal semantics that relates their run-time behavior with their provenance records. We bridge this gap by extending a previously-developed dataflow language which supports both database-style querying and workflow-style batch processing steps to produce a workflow-style provenance graph that can be explicitly queried. We define and describe the model through examples, present queries that extract other forms of provenance, and give an executable definition of the graph semantics of dataflow expressions.},
author = {Acar, Umut and Buneman, Peter and Cheney, James and {Van Den Bussche}, Jan and Kwasnikowska, Natalia and Vansummeren, Stijn},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Acar et al. - 2010 - A graph model of data and workflow provenance.pdf:pdf},
journal = {Procs. TAPP'10 workshop (Theory and Practice of Provenance)},
pages = {8},
title = {{A graph model of data and workflow provenance}},
url = {http://dl.acm.org/citation.cfm?id=1855803},
year = {2010}
}
@article{Altintas2006,
abstract = {In many data-driven applications, analysis needs to be performed on scientific information obtained from several sources and generated by computations on distributed resources. Systematic analysis of this scientific information unleashes a growing need for automated data-driven applications that also can keep track of the provenance of the data and processes with little user interaction and overhead. Such data analysis can be facilitated by the recent advancements in scientific workflow systems. A major profit when using scientific workflow systems is the ability to make provenance collection a part of the workflow. Specifically, provenance should include not only the standard data lineage information but also information about the context in which the workflow was used, execution that processed the data, and the evolution of the workflow design. In this paper we describe a complete framework for data and process provenance in the Kepler Scientific Workflow System. We outline the requirements and issues related to data and workflow provenance in a multi-disciplinary workflow system and introduce how generic provenance capture can be facilitated in Keplers actor-oriented workflow environment. We also describe the usage of the stored provenance information for efficient rerun of scientific workflows.},
author = {Altintas, Ilkay and Barney, Oscar and Jaeger-frank, Efrat},
doi = {10.1007/11890850_14},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Altintas, Barney, Jaeger-frank - 2006 - Provenance Collection Support in the Kepler Scientific Workflow System.pdf:pdf},
isbn = {3-540-46302-X, 978-3-540-46302-3},
issn = {0302-9743},
journal = {Provenance and Annotation of Data},
keywords = {kepler,provenance,workflow},
mendeley-tags = {kepler,provenance,workflow},
pages = {118--132},
pmid = {9156564},
title = {{Provenance Collection Support in the Kepler Scientific Workflow System}},
url = {http://www.springerlink.com/index/A717N1Q5714747XN.pdf},
volume = {4145},
year = {2006}
}
@article{Ramchurn2016,
abstract = {Major natural or man-made disasters such as Hurricane Katrina or the 9/11 terror attacks pose significant challenges for emergency responders. First, they have to develop an understanding of the unfolding event either using their own resources or through third-parties such as the local population and agencies. Second, based on the information gathered, they need to deploy their teams in a flexible manner, ensuring that each team performs tasks in The most effective way. Third, given the dynamic nature of a disaster space, and the uncertainties involved in performing rescue missions, information about the disaster space and the actors within it needs to be managed to ensure that responders are always acting on up-to-date and trusted information. Against this background, this paper proposes a novel disaster response system called HAC-ER. Thus HAC-ER interweaves humans and agents, both robotic and software, in social relationships that augment their individual and collective capabilities. To design HAC-ER, we involved end-users including both experts and volunteers in a several participatory design workshops, lab studies, and field trials of increasingly advanced prototypes of individual components of HAC-ER as well as the overall system. This process generated a number of new quantitative and qualitative results but also raised a number of new research questions. HAC-ER thus demonstrates how such Human-Agent Collectives (HACs) can address key challenges in disaster response. Specifically, we show how HAC-ER utilises crowdsourcing combined with machine learning to obtain most important situational awareness from large streams of reports posted by members of the public and trusted organisations. We then show how this information can inform human-agent teams in coordinating multi-UAV deployments, as well as task planning for responders on the ground. Finally, HAC-ER incorporates an infrastructure and the associated intelligence for tracking and utilising the provenance of information shared across the entire system to ensure its accountability. We individually validate each of these elements of HAC-ER and show how they perform against standard (non-HAC) baselines and also elaborate on the evaluation of the overall system.},
author = {Ramchurn, Sarvapali D. and Huynh, Trung Dong and Wu, Feng and Ikuno, Yukki and Flann, Jack and Moreau, Luc and Fischer, Joel E. and Jiang, Wenchao and Rodden, Tom and Simpson, Edwin and Reece, Steven and Roberts, Stephen and Jennings, Nicholas R.},
doi = {10.1613/jair.5098},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ramchurn et al. - 2016 - A Disaster Response System based on Human-Agent Collectives.pdf:pdf},
isbn = {145033413X},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
keywords = {disaster response,human and agents,innovative applications},
month = {dec},
pages = {661--708},
title = {{A Disaster Response System based on Human-Agent Collectives}},
url = {https://jair.org/index.php/jair/article/view/11037 http://dl.acm.org/citation.cfm?id=2772879.2772947{\%}5Cnhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84944679264{\&}partnerID=40{\&}md5=6e3ef6855fb68739e56226c582b4e9cb},
volume = {57},
year = {2016}
}
@article{Psallidas2018,
abstract = {Data lineage describes the relationship between individual input and output data items of a workflow, and has served as an integral ingredient for both traditional (e.g., debugging, auditing, data integration, and security) and emergent (e.g., interactive visualizations, iterative analytics, explanations, and cleaning) applications. The core, long-standing problem that lineage systems need to address---and the main focus of this paper---is to capture the relationships between input and output data items across a workflow with the goal to streamline queries over lineage. Unfortunately, current lineage systems either incur high lineage capture overheads, or lineage query processing costs, or both. As a result, applications, that in principle can express their logic declaratively in lineage terms, resort to hand-tuned implementations. To this end, we introduce Smoke, an in-memory database engine that neither lineage capture overhead nor lineage query processing needs to be compromised. To do so, Smoke introduces tight integration of the lineage capture logic into physical database operators; efficient, write-optimized lineage representations for storage; and optimizations when future lineage queries are known up-front. Our experiments on microbenchmarks and realistic workloads show that Smoke reduces the lineage capture overhead and streamlines lineage queries by multiple orders of magnitude compared to state-of-the-art alternatives. Our experiments on real-world applications highlight that Smoke can meet the latency requirements of interactive visualizations (e.g., {\textless}150ms) and outperform hand-written implementations of data profiling primitives.},
archivePrefix = {arXiv},
arxivId = {1801.07237},
author = {Psallidas, Fotis and Wu, Eugene},
doi = {10.14778/3184470.3184475},
eprint = {1801.07237},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Psallidas, Wu - 2018 - Smoke Fine-grained Lineage at Interactive Speed.pdf:pdf},
issn = {2150-8097},
pages = {719--732},
title = {{Smoke: Fine-grained Lineage at Interactive Speed}},
url = {http://arxiv.org/abs/1801.07237},
year = {2018}
}
@article{Scheider2018,
abstract = {In Geographic Information Systems (GIS), geoprocessing workflows allow analysts to organize their methods on spatial data in complex chains. We propose a method for expressing workflows as linked data, and for semi-automatically enriching them with semantics on the level of their operations and datasets. Linked workflows can be easily published on the Web and queried for types of inputs, results, or tools. Thus, GIS analysts can reuse their workflows in a modular way, selecting, adapting, and recom- mending resources based on compatible semantic types. Our typing approach starts from minimal annotations of workflow operations with classes of GIS tools, and then propagates data types and implicit semantic structures through the workflow using an OWL typing scheme and SPARQL rules by backtracking over GIS operations. The method is implemented in Python and is evaluated on two real-world geoprocessing workflows, generated with Esri's ArcGIS. To illustrate the potential applications of our typing method, we formulate and execute competency questions over these workflows},
author = {Scheider, Simon and Ballatore, Andrea},
doi = {10.1080/17538947.2017.1305457},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scheider, Ballatore - 2018 - Semantic typing of linked geoprocessing workflows.pdf:pdf},
issn = {17538955},
journal = {International Journal of Digital Earth},
keywords = {Geoprocessing,framework,linked data,model,modelling,moving object database,provenance,semantic,semantic typing,spatial,spatial analysis,spatio-temporal,types,workflows},
mendeley-tags = {framework,model,modelling,moving object database,provenance,semantic,spatial,spatio-temporal,types},
number = {1},
pages = {113--138},
title = {{Semantic typing of linked geoprocessing workflows}},
volume = {11},
year = {2018}
}
@article{Missier2016,
abstract = {HPCTOOLKIT is an integrated suite of tools that supports measurement, analysis, attribution, and presentation of application performance for both sequential and parallel programs. HPCTOOLKIT can pinpoint and quantify scalability bottlenecks in fully optimized parallel programs with a measurement overhead of only a few percent. Recently, new capabilities were added to HPCTOOLKIT for collecting call path profiles for fully optimized codes without any compiler support, pinpointing and quantifying bottlenecks in multithreaded programs, exploring performance information and source code using a new user interface, and displaying hierarchical spaceâ€“time diagrams based on traces of asynchronous call path samples. This paper provides an overview of HPCTOOLKIT and illustrates its utility for performance analysis of parallel applications.},
author = {Missier, Paolo and Woodman, Simon and Hiden, Hugo and Watson, Paul},
doi = {10.1002/cpe.3035},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Missier et al. - 2016 - Provenance and data differencing for workflow reproducibility analysis.pdf:pdf},
isbn = {2007015102},
issn = {15320626},
journal = {Concurrency and Computation: Practice and Experience},
keywords = {Binary analysis,Call path profiling,Execution monitoring,Performance tools,Tracing,provenance,reproducibility,workflows},
mendeley-tags = {provenance,reproducibility,workflows},
month = {mar},
number = {4},
pages = {995--1015},
pmid = {23335858},
title = {{Provenance and data differencing for workflow reproducibility analysis}},
url = {http://doi.wiley.com/10.1002/cpe.3035},
volume = {28},
year = {2016}
}
@article{Jiang2018,
abstract = {Geospatial data provenance is a fundamental issue in distributing spatial information on the Web. In the geoinformatics domain, provenance is often referred to as lineage. While the ISO 19115 lineage model is used widely in spatial data infrastructures, W3C has recommended the W3C provenance (PROV) data model for capturing and sharing provenance information on the Web. The use of these two separate efforts needs to be harmonized so that geospatial information does not remain an isolated area on the Web. Motivated by several domain use cases, we synthesize a list of provenance questions and analyze gaps between the two models in addressing these questions. Our strategy is to enrich W3C PROV with domain semantics from the ISO 19115 lineage model by suggesting ways to bridge them. A semantic mapping between the ISO lineage model and the W3C PROV model is formalized, and key issues involved are discussed. Use cases illustrate the applicability of the approach.},
author = {Jiang, Liangcun and Yue, Peng and Kuhn, Werner and Zhang, Chenxiao and Yu, Changhui and Guo, Xia},
doi = {10.1016/j.cageo.2018.05.001},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2018 - Advancing interoperability of geospatial data provenance on the web Gap analysis and strategies.pdf:pdf},
issn = {00983004},
journal = {Computers and Geosciences},
keywords = {Data lineage,Geospatial data provenance,ISO 19115,Interoperability,Lineage model,Prov,Provenance,Spatial,W3C PROV},
mendeley-tags = {Data lineage,Interoperability,Prov,Provenance,Spatial},
number = {March},
pages = {21--31},
publisher = {Elsevier Ltd},
title = {{Advancing interoperability of geospatial data provenance on the web: Gap analysis and strategies}},
url = {https://doi.org/10.1016/j.cageo.2018.05.001},
volume = {117},
year = {2018}
}
@inproceedings{Batlajery2018,
address = {London, United Kingdom},
author = {Batlajery, Belfrit Victor and Weal, Mark and Chapman, Adriane and Moreau, Luc},
booktitle = {Provenance Week '18: 7th International Provenance And Annotation Workshop},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Batlajery et al. - 2018 - Belief Propagation Through Provenance Graphs.pdf:pdf},
keywords = {causality,food,probability,provenance},
mendeley-tags = {causality,food,probability,provenance},
title = {{Belief Propagation Through Provenance Graphs}},
url = {https://kclpure.kcl.ac.uk/portal/en/publications/belief-propagation-through-provenance-graphs(c1b7a54d-4e9c-4a9f-8d7d-cce4a6b1e4ab).html},
year = {2018}
}
@article{Stamatogiannakis2017,
abstract = {Information produced by Internet applications is inherently a result of processes that are executed locally. Think of a web server that makes use of a CGI script, or a content management system where a post was first edited using a word processor. Given the impact of these processes to the content published online, a consumer of that information may want to understand what those impacts were. For example, understanding from where text was copied and pasted to make a post, or if the CGI script was updated with the latest security patches, may all influence the confidence on the published content. Capturing and exposing this information provenance is thus important in order to ascertaining trust to online content. Furthermore, providers of internet applications may wish to have access to the same information for debugging or audit purposes. For processes following a rigid structure (such as databases or workflows), disclosed provenance systems have been developed that efficiently and accurately capture the provenance of the produced data. However, accurately capturing provenance from unstructured processes, e.g. user-interactive computing used to produce web content, remains a problem to be tackled. In this paper, we address the problem of capturing and exposing provenance from unstructured processes. Our approach, called PROV 2R (PROVenance Record and Replay) is composed of two parts: 1) the decoupling of provenance analysis from its capture; and 2) the capture of high fidelity provenance from unmodified programs. We use techniques originating in the security and reverse engineering communities, namely, record and replay and taint tracking. Taint tracking fundamentally addresses the data provenance problem, but is impractical to apply at runtime due to extremely high overhead. With a number of case studies we demonstrate that PROV 2R enables the use of taint analysis for high fidelity provenance capture, while keeping the runtime overhead at manageable levels. In addition, we show how captured information can be represented using the W3C PROV provenance model for exposure on the Web.},
author = {Stamatogiannakis, Manolis and Athanasopoulos, Elias and Bos, Herbert and Groth, Paul},
doi = {10.1145/3062176},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stamatogiannakis et al. - 2017 - PROV2R Practical Provenance Analysis of Unstructured Processes.pdf:pdf},
issn = {15576051},
journal = {ACM Transactions on Internet Technology (TOIT)},
keywords = {Automation,Decoupling,Human in the loop,Interactive,Manual,Prov,Provenance,Strucutred,Unstructured,Workflows},
mendeley-tags = {Automation,Decoupling,Human in the loop,Interactive,Manual,Prov,Provenance,Strucutred,Unstructured,Workflows},
number = {4},
title = {{PROV2R: Practical Provenance Analysis of Unstructured Processes}},
volume = {17},
year = {2017}
}
@article{Prabhune2017,
abstract = {Enabling provenance interoperability by analyzing heterogeneous provenance information from different scientific workflow management systems is a novel research topic. With the advent of the ProvONE model, it is now possible to model both the prospective as well as the retrospective provenance in a single provenance model. Scientific workflows are composed using a declarative definition language, such as BPEL, SCUFL/t2flow, or MoML. Associated with the execution of a workflow is its corresponding provenance that is modeled and stored in the data model specified by the workflow system. However, sharing of provenance generated by heterogeneous workflows is a challenging task and prevents the aggregate analysis and comparison of workflows and their associated provenance. To address these challenges, this paper introduces a ProvONE-based Provenance Interoperability Framework that completely automates the modeling of provenance from heterogeneous WfMSs by: (a) automatically translating the scientific workflows to their equivalent representation in a ProvONE prospective graph using the Prov2ONE algorithm, (b) enriching the ProvONE prospective graph with the retrospective provenance exported by the WfMSs, and (c) native support for storing the ProvONE provenance graphs in a Resource Description Framework triplestore that supports the SPARQL query language for querying and retrieving ProvONE graphs. The Prov2ONE algorithm is based on a set of vocabulary translation rules between workflow specifications and the ProvONE model. The correctness and completeness proof of the algorithm is shown and its complexity is analyzed. Moreover, to demonstrate the practical applicability of the complete framework, ProvONE graphs for workflows defined in BPEL, SCUFL, and MoML are generated. Finally, the provenance challenge queries are extended with six additional queries for retrieving the provenance modeled in ProvONE.},
author = {Prabhune, Ajinkya and Zweig, Aaron and Stotzka, Rainer and Hesser, J{\"{u}}rgen and Gertz, Michael},
doi = {10.1007/s10619-017-7216-y},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Prabhune et al. - 2017 - P-PIF a ProvONE provenance interoperability framework for analyzing heterogeneous workflow specifications and p.pdf:pdf},
issn = {15737578},
journal = {Distributed and Parallel Databases},
keywords = {Prospective provenance,ProvONE provenance model,Provenance interoperability,RDF,Retrospective provenance,SPARQL,Workflow Management System},
number = {1},
pages = {1--46},
publisher = {Springer US},
title = {{P-PIF: a ProvONE provenance interoperability framework for analyzing heterogeneous workflow specifications and provenance traces}},
url = {https://doi.org/10.1007/s10619-017-7216-y},
volume = {36},
year = {2017}
}
@article{Mayer2015,
author = {Mayer, Rudolf and Rauber, Andreas},
doi = {10.1109/eScience.2015.58},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mayer, Rauber - 2015 - A quantitative study on the re-executability of publicly shared scientific workflows.pdf:pdf},
isbn = {9781467393256},
journal = {Proceedings - 11th IEEE International Conference on eScience, eScience 2015},
keywords = {Repeatability,Reproducibility,Research workflows,Taverna},
pages = {312--321},
title = {{A quantitative study on the re-executability of publicly shared scientific workflows}},
year = {2015}
}
@inproceedings{Dolan-Gavitt2015,
abstract = {We present PANDA, an open-source tool that has been purpose-built to support whole system reverse engineering. It is built upon the QEMU whole system emulator, and so analyses have access to all code executing in the guest and all data. PANDA adds the ability to record and replay executions, enabling iterative, deep, whole system analyses. Further, the replay log files are compact and shareable, allowing for repeatable experiments. A nine billion instruction boot of FreeBSD, e.g., is represented by only a few hundred MB. PANDA leverages QEMU's support of thirteen different CPU architectures to make analyses of those diverse instruction sets possible within the LLVM IR. In this way, PANDA can have a single dynamic taint analysis, for example, that precisely supports many CPUs. PANDA analyses are written in a simple plugin architecture which includes a mechanism to share functionality between plug- ins, increasing analysis code re-use and simplifying complex analysis development. We demonstrate PANDA's effectiveness via a number of use cases, including enabling an old but legitimately purchased game to run despite a lost CD key, in-depth diagnosis of an Internet Explorer crash, and uncovering the censorship activities and mechanisms of an IM client.},
address = {New York, New York, USA},
author = {Dolan-Gavitt, Brendan and Hodosh, Josh and Hulin, Patrick and Leek, Tim and Whelan, Ryan},
booktitle = {Proceedings of the 5th Program Protection and Reverse Engineering Workshop - PPREW-5},
doi = {10.1145/2843859.2843867},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dolan-Gavitt et al. - 2015 - Repeatable Reverse Engineering with PANDA.pdf:pdf},
isbn = {9781450336420},
keywords = {instrumentation,introspection,record,replay},
pages = {1--11},
publisher = {ACM Press},
title = {{Repeatable Reverse Engineering with PANDA}},
url = {http://dl.acm.org/citation.cfm?doid=2843859.2843867},
year = {2015}
}
@article{Vardigan2014,
abstract = {With support from the National Science Foundation, two long-running social science studies - the American National Election Study and the General Social Survey - partnered with the Inter-university Consortium for Political and Social Research (ICPSR) and NORC at the University of Chicago to improve their metadata and build demonstration tools to illustrate the value of structured, machine-actionable metadata. The partnership also involved evaluating the studies' data collection workflows to determine where in the data life cycle metadata could be captured at source to avoid metadata loss and costly procedures to recreate the metadata later. This article reports on the experience and knowledge gained over the course of the project and also includes recommendations for others undertaking similar work.},
author = {Vardigan, Mary and Donakowski, Darrell and Heus, Pascal and Ionescu, Sanda and Rotondo, Julia},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vardigan et al. - 2014 - Creating rich, structured metadata lessons learned in the Metadata Portal Project.pdf:pdf},
issn = {07391137},
journal = {IASSIST Quarterly},
keywords = {DDI,Provenance,metadata},
mendeley-tags = {DDI,Provenance},
number = {3},
pages = {15--20},
title = {{Creating rich, structured metadata: lessons learned in the Metadata Portal Project}},
url = {http://www.iassistdata.org/sites/default/files/iqvol38{\_}3{\_}vardigan.pdf},
volume = {38},
year = {2014}
}
@article{Cheney2007,
abstract = {Provenance is information that aids understanding and troubleshooting database queries by explaining the results in terms of the input. Slicing is a program analysis technique for debugging and understand- ing programs that has been studied since the early 1980s, in which program results are explained in terms of parts of the program that contributed to the results. This paper will briefly review ideas and techniques from program slicing and show how they might be useful for improving our understanding of provenance in databases.},
author = {Cheney, James},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheney - 2007 - Program Slicing and Data Provenance.pdf:pdf},
journal = {IEEE Data Engineering Bulletin},
number = {4},
pages = {22--28},
title = {{Program Slicing and Data Provenance}},
volume = {30},
year = {2007}
}
@incollection{Saenz-Adan2018,
author = {S{\'{a}}enz-Ad{\'{a}}n, Carlos and P{\'{e}}rez, Beatriz and Huynh, Trung Dong and Moreau, Luc},
booktitle = {44th International Conference on Current Trends in Theory and Practice of Computer Science},
doi = {10.1007/978-3-319-73117-9_47},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/S{\'{a}}enz-Ad{\'{a}}n et al. - 2018 - UML2PROV Automating Provenance Capture in Software Engineering.pdf:pdf},
pages = {667--681},
title = {{UML2PROV: Automating Provenance Capture in Software Engineering}},
url = {https://kclpure.kcl.ac.uk/portal/en/publications/uml2prov-automating-provenance-capture-in-software-engineering(3550fbb4-1d16-42db-97ab-0466a8c67683).html http://link.springer.com/10.1007/978-3-319-73117-9{\_}47},
year = {2018}
}
@incollection{Ji2016,
abstract = {The World Wide Web evolves into a Web of Data, a huge, globally distributed dataspace that contains a rich body of machine-processable information from a virtually unbound set of providers covering a wide range of topics. However, due to the openness of the Web little is known about who created the data and how. The fact that a large amount of the data on the Web is derived by replication, query processing, modification, or merging raises concerns of information quality. Poor quality data may propagate quickly and contaminate the Web of Data. Provenance information about who created and published the data and how, provides the means for quality assessment. This paper takes a first step towards creating a quality-aware Web of Data: we present approaches to integrate provenance information into the Web of Data and we illustrate how this information can be consumed. In particular, we introduce a vocabulary to describe provenance of Web data as metadata and we discuss possibilities to make such provenance metadata accessible as part of the Web of Data. Furthermore, we describe how this metadata can be queried and consumed to identify outdated information.},
archivePrefix = {arXiv},
arxivId = {1406.2495},
author = {Ji, Yang and Lee, Sangho and Lee, Wenke},
doi = {10.1007/978-3-319-40593-3_1},
eprint = {1406.2495},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ji, Lee, Lee - 2016 - RecProv Towards Provenance-Aware User Space Record and Replay.pdf:pdf},
isbn = {978-3-642-34221-9},
issn = {03029743},
keywords = {provenance capturing},
pages = {3--15},
pmid = {9156564},
title = {{RecProv: Towards Provenance-Aware User Space Record and Replay}},
url = {http://link.springer.com/10.1007/978-3-319-40593-3{\_}1},
volume = {7525},
year = {2016}
}
@article{Kim2007,
abstract = {Our research focuses on creating and executing large-scale scientific workflows that often involve thousands of computations over distributed, shared resources. We describe an approach to workflow creation and refinement that uses semantic representations to 1) describe complex scientific applications in a data-independent manner, 2) automatically generate workflows of computations for given data sets, and 3) map the workflows to available computing resources for efficient execution. Our approach is implemented in the Wings/Pegasus workflow system and has been demonstrated in a variety of scientific application domains. This paper illustrates the application-level provenance information generated Wings during workflow creation and the refinement provenance by the Pegasus mapping system for execution over grid computing environments. We show how this information is used in answering the queries of the First Provenance Challenge.},
author = {Kim, Jihie and Deelman, Ewa and Gil, Yolanda and Mehta, Gaurang and Ratnakar, Varun and Rey, Marina},
doi = {10.1002/cpe.1228},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2007 - Provenance Trails in the Wings Pegasus System Wings Pegasus Creating and Executing Large Workflows.pdf:pdf},
issn = {15320634},
journal = {Information Sciences},
keywords = {large scientific workflows,refinement provenance,semantic metadata,workflow mapping,workflow provenance,workflow validation},
number = {5},
pages = {1--11},
title = {{Provenance Trails in the Wings / Pegasus System Wings / Pegasus : Creating and Executing Large Workflows}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.121.2529},
volume = {20},
year = {2007}
}
@article{Herschel2017,
author = {Herschel, Melanie and Diestelk{\"{a}}mper, Ralf and {Ben Lahmar}, Houssem},
doi = {10.1007/s00778-017-0486-1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Herschel, Diestelk{\"{a}}mper, Ben Lahmar - 2017 - A survey on provenance What for What form What from.pdf:pdf},
issn = {0949877X},
journal = {VLDB Journal},
keywords = {Data provenance,Provenance applications,Provenance capture,Provenance requirements,Provenance types,Survey,Workflow provenance},
number = {6},
pages = {881--906},
publisher = {Springer Berlin Heidelberg},
title = {{A survey on provenance: What for? What form? What from?}},
volume = {26},
year = {2017}
}
@article{Moreau2018,
abstract = {PROV-TEMPLATE is a declarative approach that enables designers and programmers to design and generate provenance compatible with the prov standard of the World Wide Web Consortium. Designers specify the topology of the provenance to be generated by composing templates, which are provenance graphs containing variables, acting as placeholders for values. Programmers write programs that log values and package them up in sets of bindings, a data structure associating variables and values. An expansion algorithm generates instantiated provenance from templates and sets of bindings in any of the serialisation formats supported by prov. A quantitative evaluation shows that sets of bindings have a size that is typically 40 percent of that of expanded provenance templates and that the expansion algorithm is suitably tractable, operating in fractions of milliseconds for the type of templates surveyed in the article. Furthermore, the approach shows four significant software engineering benefits: separation of responsibilities, provenance maintenance, potential runtime checks and static analysis, and provenance consumption. The article gathers quantitative data and qualitative benefits descriptions from four different applications making use of prov-template. The system is implemented and released in the open-source library ProvToolbox for provenance processing.},
author = {Moreau, Luc and Batlajery, Belfrit Victor and Huynh, Trung Dong and Michaelides, Danius and Packer, Heather},
doi = {10.1109/TSE.2017.2659745},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moreau et al. - 2018 - A Templating System to Generate Provenance.pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {PROV,Provenance,provenance generation,template},
number = {2},
pages = {103--121},
title = {{A Templating System to Generate Provenance}},
volume = {44},
year = {2018}
}
@article{Hettne2014,
abstract = {One of the main challenges for biomedical research lies in the computer-assisted integrative study of large and increasingly complex combinations of data in order to understand molecular mechanisms. The preservation of the materials and methods of such computational experiments with clear annotations is essential for understanding an experiment, and this is increasingly recognized in the bioinformatics community. Our assumption is that offering means of digital, structured aggregation and annotation of the objects of an experiment will provide necessary meta-data for a scientist to understand and recreate the results of an experiment. To support this we explored a model for the semantic description of a workflow-centric Research Object (RO), where an RO is defined as a resource that aggregates other resources, e.g., datasets, software, spreadsheets, text, etc. We applied this model to a case study where we analysed human metabolite variation by workflows.},
archivePrefix = {arXiv},
arxivId = {1311.2789},
author = {Hettne, Kristina M. and Dharuri, Harish and Zhao, Jun and Wolstencroft, Katherine and Belhajjame, Khalid and Soiland-Reyes, Stian and Mina, Eleni and Thompson, Mark and Cruickshank, Don and Verdes-Montenegro, Lourdes and Garrido, Julian and {De Roure}, David and Corcho, Oscar and Klyne, Graham and {Van Schouwen}, Reinout and Hoen't, Peter A.C. and Bechhofer, Sean and Goble, Carole and Roos, Marco},
doi = {10.1186/2041-1480-5-41},
eprint = {1311.2789},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hettne et al. - 2014 - Structuring research methods and data with the research object model Genomics workflows as a case study.pdf:pdf},
isbn = {2041-1480},
issn = {20411480},
journal = {Journal of Biomedical Semantics},
keywords = {Digital libraries,Genome wide association study,Scientific workflows,Semantic web models,case study,workflow},
mendeley-tags = {case study,workflow},
number = {1},
pages = {1--16},
pmid = {25276335},
title = {{Structuring research methods and data with the research object model: Genomics workflows as a case study}},
volume = {5},
year = {2014}
}
@article{Hall2017,
abstract = {{\textcopyright} by the author(s). Research on provenance, which focuses on different ways to describe and record the history of changes and advances made throughout an analysis process, is an integral part of visual analytics. This paper focuses on providing the provenance of insight and rationale through visualizations while emphasizing, first, that this entails a profound understanding of human cognition and reasoning and that, second, the special nature of spatiotemporal data needs to be acknowledged in this process. A recently proposed human reasoning framework for spatiotemporal analysis, and four guidelines for the creation of visualizations that provide the provenance of insight and rationale published in relation to that framework, work as a starting point for this paper. While these guidelines are quite abstract, this paper set out to create a set of more concrete guidelines. On the basis of a review of available provenance solutions, this paper identifies a set of key features that are of relevance when providing the provenance of insight and rationale and, on the basis of these features, produces a new set of complementary guidelines that are more practically oriented than the original ones. Together, these two sets of guidelines provide both a theoretical and practical approach to the problem of providing the provenance of insight and rationale. Providing these kinds of guidelines represents a new approach in provenance research.},
author = {Hall, A. and Ahonen-Rainio, P. and Virrantaus, K.},
doi = {10.5311/JOSIS.2017.15.337},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hall, Ahonen-Rainio, Virrantaus - 2017 - Insight provenance for spatiotemporal visual analytics Theory, review, and guidelines.pdf:pdf},
issn = {1948660X},
journal = {Journal of Spatial Information Science},
keywords = {Cognition,Framework,Guidelines,Provenance,Reasoning,Review,Spatiotemporal,Visual analytics,Visualization},
number = {15},
pages = {65--88},
title = {{Insight provenance for spatiotemporal visual analytics: Theory, review, and guidelines}},
volume = {15},
year = {2017}
}
@article{Ornelas2018,
author = {Ornelas, Tatiane and Braga, Regina and David, Jos{\'{e}} Maria N. and Campos, Fernanda and Castro, Gabriella},
doi = {10.1002/cpe.4366},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ornelas et al. - 2018 - Provenance data discovery through Semantic Web resources.pdf:pdf},
issn = {15320626},
journal = {Concurrency and Computation: Practice and Experience},
keywords = {OPM,Semantic Web,completeness rules,ontology,provenance},
number = {6},
pages = {e4366},
title = {{Provenance data discovery through Semantic Web resources}},
url = {http://doi.wiley.com/10.1002/cpe.4366},
volume = {30},
year = {2018}
}
@book{Swartz2013,
author = {Swartz, Aaron},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Swartz - 2013 - Aaron Swartz's A Programmable Web.pdf:pdf},
isbn = {9781627051699},
title = {{Aaron Swartz's A Programmable Web}},
year = {2013}
}
@phdthesis{Fernando2017,
author = {Fernando, Tharidu},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fernando - 2017 - WorkflowDSL Scalable Workflow Execution with Provenance.pdf:pdf},
keywords = {provenance,workflow},
title = {{WorkflowDSL: Scalable Workflow Execution with Provenance}},
type = {Masters},
url = {http://www.diva-portal.org/smash/get/diva2:1149093/FULLTEXT01.pdf},
year = {2017}
}
@article{Gregory2011,
abstract = {This paper introduces DDI to those coming from national statistics institutes (NSIs). While there is a large amount of information regarding DDI available today, sometimes it is difficult to know where to start, and much of it comes from domains which are not familiar to those working with official statistics. Here, we attempt to characterize the flavors and uses of DDI, give some general background on the standards organization (the DDI Alliance), describe available tools, and relate the DDI to other initiatives and standards which are more familiar to this audience.},
author = {Gregory, Arofan},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gregory - 2011 - The Data Documentation Initiative ( DDI ) An Introduction for National Statistical Institutes.pdf:pdf},
keywords = {DDI,Provenance},
mendeley-tags = {DDI,Provenance},
number = {July},
pages = {1--10},
title = {{The Data Documentation Initiative ( DDI ): An Introduction for National Statistical Institutes}},
year = {2011}
}
@article{Ikeda2010,
abstract = {Panda (for Provenance and Data) is a new project whose goal is to address some limitations in existing provenance systems. This short paper describes our overall plans for Panda, including: a model that fully integrates data-based and process-based provenance; a set of built-in operators for exploiting provenance after it has been captured; an ad-hoc query language over provenance together with data; supporting the range from fine-grained to coarse-grained provenance; and addressing optimization problems involving eager versus lazy evaluation and data caching.},
author = {Ikeda, Robert and Widom, Jennifer},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ikeda, Widom - 2010 - Panda A System for Provenance and Data.pdf:pdf},
journal = {Proceedings of the 2nd USENIX Workshop on the Theory and Practice of Provenance TaPP'10},
keywords = {Provenance},
mendeley-tags = {Provenance},
pages = {1--8},
title = {{Panda: A System for Provenance and Data}},
url = {http://www.usenix.org/events/tapp10/tech/full{\_}papers/ikeda.pdf},
volume = {33},
year = {2010}
}
@article{Massi2018,
abstract = {Provenance is the foundation of data quality, usually implemented by automatically capturing the trace of data manipulation over space and time. In healthcare, provenance becomes critical since it encompasses both clinical research and patient safety. In this proposal we aim at exploiting and innovating existing health IT deployments by enabling data provenance queries for all kind of clinical information from anywhere. The proposed technical solution exploits the novelty and the peer-to-peer fashion of the blockchain technology and smart-contracts to instrument international standards such as IHE and HL7 with a provenance system robust to fraudulences.},
author = {Massi, Massimiliano and Miladi, Abdallah and Margheri, Andrea and Sassone, Vladimiro and Rosenzweig, Jason},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Massi et al. - 2018 - Using PROV and Blockchain to Achieve Health Data Provenance.pdf:pdf},
keywords = {Blockchain,Provenance},
mendeley-tags = {Blockchain,Provenance},
pages = {1--24},
title = {{Using PROV and Blockchain to Achieve Health Data Provenance}},
url = {https://eprints.soton.ac.uk/421292/},
year = {2018}
}
@article{Buneman2001,
abstract = {With the proliferation of database views and curated databases, the issue of data provenance - where a piece of data came from and the process by which it arrived in the database - is becoming increasingly important, especially in scientific databases where understanding provenance is crucial to the accuracy and currency of data. In this paper we describe an approach to computing provenance when the data of interest has been created by a database query. We adopt a syntactic approach and present results for a general data model that applies to relational databases as well as to hierarchical data such as XML. A novel aspect of our work is a distinction between â€œwhyâ€ provenance (refers to the source data that had some influence on the existence of the data) and â€œwhereâ€ provenance (refers to the location(s) in the source databases from which the data was extracted).},
author = {Buneman, Peter and Khanna, Sanjeev and Tan, W and Wang-Chiew, Tan},
doi = {10.1007/3-540-44503-X_20},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buneman et al. - 2001 - Why and Where A Characterization of Data Provenance.pdf:pdf},
isbn = {9783540414568},
issn = {02698463},
journal = {Proceedings of International Conference on Database Theory (ICDT)},
pages = {316--330},
pmid = {7011450},
title = {{Why and Where: A Characterization of Data Provenance}},
volume = {1973},
year = {2001}
}
@article{Wu2017,
author = {Wu, Dongyao and Sakr, Sherif and Zhu, Liming},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Sakr, Zhu - 2017 - HDM Optimized Big Data Processing with Data Provenance.pdf:pdf},
isbn = {9783893180738},
keywords = {big data,data flow optimization,nicta,provenance manage-},
mendeley-tags = {nicta},
pages = {530--533},
title = {{HDM : Optimized Big Data Processing with Data Provenance}},
year = {2017}
}
@article{Wu2016,
abstract = {Many real-world data analysis scenarios require pipelining and integration of multiple (big) data-processing and data-analytics jobs, which often execute in heterogeneous environments, such as MapReduce; Spark; or R, Python, or Bash scripts. Such a pipeline requires much glue code to get data across environments. Maintaining and evolving these pipelines are difficult. Pipeline frameworks that try to solve such problems are usually built on top of a single environment. They might require rewriting the original job to take into account a new API or paradigm. The Pipeline61 framework supports the building of data pipelines involving heterogeneous execution environments. It reuses the existing code of the deployed jobs in different environments and provides version control and dependency management that deals with typical software engineering issues. A real-world case study shows its effectiveness. This article is part of a special issue on Software Engineering for Big Data Systems.},
author = {Wu, Dongyao and Zhu, Liming and Xu, Xiwei and Sakr, Sherif and Sun, Daniel and Lu, Qinghua},
doi = {10.1109/MS.2016.35},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2016 - Building pipelines for heterogeneous execution environments for big data processing.pdf:pdf},
isbn = {0740-7459 VO - 33},
issn = {07407459},
journal = {IEEE Software},
keywords = {MapReduce,Pipeline61,Spark,big data,dag,nicta,pipeline,pipeline61,software development,software engineering},
mendeley-tags = {dag,nicta,pipeline61},
number = {2},
pages = {60--67},
title = {{Building pipelines for heterogeneous execution environments for big data processing}},
volume = {33},
year = {2016}
}
@article{DeRoure2009,
abstract = {In this paper we suggest that the full scientific potential of workflows will be achieved through mechanisms for sharing and collaboration, empowering scientists to spread their experimental protocols and to benefit from those of others. To facilitate this process we have designed and built them yExperiment Virtual Research Environment for collaboration and sharing of workflows and experiments. In contrast to systems which simply make workflows available,m yExperiment provides mechanisms to support the sharing of workflows within and across multiple communities. It achieves this by adopting a social web approach which is tailored to the particular needs of the scientist. We present the motivation, design and realisation ofm yExperiment. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {{De Roure}, David and Goble, Carole and Stevens, Robert},
doi = {10.1016/j.future.2008.06.010},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Roure, Goble, Stevens - 2009 - The design and realisation of the myExperiment Virtual Research Environment for social sharing of work.pdf:pdf},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Collaborative computing,Scientific workflow,Taverna workflow workbench,Virtual research environment,Workflow management,myexperiment,provenanace,taverna,workflow},
mendeley-tags = {myexperiment,provenanace,taverna,workflow},
number = {5},
pages = {561--567},
publisher = {Elsevier B.V.},
title = {{The design and realisation of the myExperiment Virtual Research Environment for social sharing of workflows}},
url = {http://dx.doi.org/10.1016/j.future.2008.06.010},
volume = {25},
year = {2009}
}
@article{Wang2008,
abstract = {GIS (Geographic Information Systems) play an important role to acquire and communicate geospatial knowledge based on spatial data and the use of spatial analysis, modeling, and visualization. The assurance of the validity and quality of spatial data handling and analysis remains a great challenge, in part, because of sophisticated procedures are often required for collaborative geospatial problem-solving and decision making. These procedures, when specified as knowledge derivation workflows, require carefully configured parameters and spatiotemporal specifications guided by specific contexts and purposes. The information of spatial data lineage and related analysis workflow is defined as spatial provenance in this research. Such information is often not well recorded or managed during spatial data handling and related analysis. This paper presents a provenance-aware GIS architecture that incorporates spatial provenance to address this shortcoming and facilitate the assurance of validity and quality of spatial data handling and analysis. Spatial provenance in this architecture is generated and managed to allow queries on data lineage and workflow information to support geospatial problemsolving. Basic elements of spatial provenance are captured using a spatial provenance model. The illustration of the provenanceaware GIS architecture and its proof-of-concept implementation reveals the similarity and difference in the use of spatial provenance in GIS applications. Overall, the architecture and implementation described in the paper demonstrates the necessity and feasibility of introducing provenance into GIS. {\textcopyright} 2008 ACM.},
author = {Wang, S. and Padmanabhan, A. and Myers, J. D. and Tang, W. and Liu, Y.},
doi = {10.1145/1463434.1463515},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2008 - Towards provenance-aware geographic information systems.pdf:pdf},
isbn = {9781605583235},
journal = {GIS: Proceedings of the ACM International Symposium on Advances in Geographic Information Systems},
keywords = {gis,open provenance model,spatial provenance,web services},
pages = {483--486},
title = {{Towards provenance-aware geographic information systems}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-70449732749{\&}partnerID=40{\&}md5=7a6e071656b3add2c1834a7ff119efea},
year = {2008}
}
@article{Bowers2006,
abstract = {Integrated provenance support promises to be a chief advantage of scientific workflow systems over script-based alternatives. While it is often recognized that information gathered during scientific workflow execution can be used automatically to increase fault tolerance (via checkpointing) and to optimize performance (by reusing intermediate data products in future runs), it is perhaps more significant that provenance information may also be used by scientists to reproduce results from earlier runs, to explain unexpected results, and to prepare results for publication. Current workflow systems offer little or no direct support for these "scientist-oriented" queries of provenance information. Indeed the use of advanced execution models in scientific workflows (e.g., process networks, which exhibit pipeline parallelism over streaming data) and failure to record certain fundamental events such as state resets of processes, can render existing provenance schemas useless for scientific applications of provenance. We develop a simple provenance model that is capable of supporting a wide range of scientific use cases even for complex models of computation such as process networks. Our approach reduces these use cases to database queries over event logs, and is capable of reconstructing complete data and invocation dependency graphs for a workflow run. Abstract. Integrated provenance support promises to be a chief advan-tage of scientific workflow systems over script-based alternatives. While it is often recognized that information gathered during scientific work-flow execution can be used automatically to increase fault tolerance (via checkpointing) and to optimize performance (by reusing intermediate data products in future runs), it is perhaps more significant that prove-nance information may also be used by scientists to reproduce results from earlier runs, to explain unexpected results, and to prepare results for publication. Current workflow systems offer little or no direct support for these " scientist-oriented " queries of provenance information. Indeed the use of advanced execution models in scientific workflows (e.g., pro-cess networks, which exhibit pipeline parallelism over streaming data) and failure to record certain fundamental events such as state resets of processes, can render existing provenance schemas useless for scientific applications of provenance. We develop a simple provenance model that is capable of supporting a wide range of scientific use cases even for complex models of computation such as process networks. Our approach reduces these use cases to database queries over event logs, and is capa-ble of reconstructing complete data and invocation dependency graphs for a workflow run.},
author = {Bowers, Shawn and Mcphillips, Timothy and Ludascher, Bertram and Cohen, Shirley and Davidson, Susan B and Lud{\"{a}}scher, Bertram},
doi = {10.1007/11890850_15},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bowers et al. - 2006 - A Model for User-Oriented Data Provenance in Pipelined Scientific Workflows.pdf:pdf},
isbn = {9783540463023},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
keywords = {Kepler,bioinformatics,databases,workflows},
number = {4145},
pages = {133--147},
pmid = {9199103},
title = {{A Model for User-Oriented Data Provenance in Pipelined Scientific Workflows}},
url = {http://dx.doi.org/10.1007/11890850{\_}15{\%}5Cnhttp://repository.upenn.edu/cis{\_}papers/290},
volume = {4145},
year = {2006}
}
@article{Dey2012,
author = {Dey, Saumen and K{\"{o}}hler, Sven and Bowers, Shawn and Lud{\"{a}}scher, Bertram},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dey et al. - 2012 - Datalog As a Lingua Franca for Provenance Querying and Reasoning.pdf:pdf},
journal = {Proceedings of the 4th USENIX Conference on Theory and Practice of Provenance},
pages = {13},
title = {{Datalog As a Lingua Franca for Provenance Querying and Reasoning}},
url = {http://dl.acm.org/citation.cfm?id=2342875.2342888},
year = {2012}
}
@article{Cox2015,
abstract = {The PROV data model is becoming accepted as a flexible and robust tool for formalizing information relating to the production of documents and datasets. Provenance stores based on the PROV-O implementation are appearing in support of scientific data workflows. However, the scope of PROV does not have to be limited to digital or information assets. For example, specimens typically undergo complex preparation sequences prior to actual observations and measurements, and it is important to record this to ensure reproducibility and to enable assessment of the reliability of data produced. PROV provides a flexible solution, allowing a comprehensive trace of predecessor entities and transformations at any level of detail. In this paper we demonstrate the use of PROV for describing specimens managed for scientific observations. Two examples are considered: a geological sample which undergoes a typical preparation process for measurements of the concentration of a particular chemical substance, and the collection, taxonomic classification and eventual publication of an insect specimen. We briefly compare PROV with related work. 1. INTRODUCTION The concept of provenance was developed in the art, museums and archives community, referring to a record of the history of an artefact. Tracing the history of ownership and custodianship is an important means of determining the authenticity of rare or unique items. More recently, the same term has been applied to the lineage of information objects -particularly datasets, imagery, etc. These may have a complex history with multiple data-processing steps, the details of which are important in evaluating the quality or fitness for a particular purpose, and also for establishing reproducibility which is a core tenet of empirical science. In this setting, a 'provenance trace' can be seen as the record of an instantiated 'workflow'. The W3C PROV model [6,9] harmonizes a number of earlier treatments (in particular PML and OPM) and is becoming accepted as the basis for formalizing information relating to the production of documents and datasets. Provenance stores based on the PROV-O implementation, such as PROMS [1], are appearing in support of scientific data workflows. In the context of technical and scientific collections, where specimens (biological, water, soil and rock) are managed to support subsequent observations, chain of custody concerns do arise, particularly in relation to forensic applications, and also where there are financial implications from results of observations on samples, such as assays on mineral exploration specimens. But the key feature of technical and scientific specimens is the preparation process, generating a sequence of samples -some of which only exist temporarily -which are related through various processing activities, in support of the observational requirements. There is enormous variety in the process-chains related to these, both between and within disciplines. In fact, the design of new sequences is a key activity in empirical science. The PROV ontology -which abstracts all possible processing chains into a single high level model whereby the production and transformation of Entities is through time-bounded Activities, under the influence or control of Agents -appears to provide a framework that can satisfy all of the relevant concerns, either directly or through minor specializations within a basic framework.},
author = {Cox, S. J. D. and Car, N. J.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cox, Car - 2015 - PROV and Real Things.pdf:pdf},
journal = {MODSIM2015, 21st International Congress on Modelling and Simulation},
keywords = {PROV,Physical,Provenance,sampling,specimen},
mendeley-tags = {Physical,Provenance},
number = {November},
pages = {620--626},
title = {{PROV and Real Things}},
url = {http://mssanz.org.au/modsim2015/C4/cox.pdf},
year = {2015}
}
@inproceedings{Zhao2012,
author = {Zhao, Jun and Gomez-Perez, Jose Manuel and Belhajjame, Khalid and Klyne, Graham and Garcia-Cuesta, Esteban and Garrido, Aleix and Hettne, Kristina and Roos, Marco and {De Roure}, David and Goble, Carole},
booktitle = {2012 IEEE 8th International Conference on E-Science},
doi = {10.1109/eScience.2012.6404482},
isbn = {978-1-4673-4466-1},
keywords = {practice,taverna,workflows},
mendeley-tags = {practice,taverna,workflows},
month = {oct},
pages = {1--9},
publisher = {IEEE},
title = {{Why workflows break - Understanding and combating decay in Taverna workflows}},
url = {http://ieeexplore.ieee.org/document/6404482/},
year = {2012}
}
@book{Perez2018,
author = {P{\'{e}}rez, Beatriz and Rubio, Julio and S{\'{a}}enz-Ad{\'{a}}n, Carlos},
booktitle = {Knowledge and Information Systems},
doi = {10.1007/s10115-018-1164-3},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/P{\'{e}}rez, Rubio, S{\'{a}}enz-Ad{\'{a}}n - 2018 - A systematic review of provenance systems.pdf:pdf},
isbn = {1011501811643},
issn = {0219-1377},
keywords = {Provenance systems,Provenance aspects,Computer sci},
pages = {1--49},
publisher = {Springer London},
title = {{A systematic review of provenance systems}},
url = {http://link.springer.com/10.1007/s10115-018-1164-3},
year = {2018}
}
@article{Mondelli2018,
abstract = {Advances in sequencing techniques have led to exponential growth in biological data, demanding the development of large-scale bioinformatics experiments. Because these experiments are computation- and data-intensive, they require high-performance computing (HPC) techniques and can benefit from specialized technologies such as Scientific Workflow Management Systems (SWfMS) and databases. In this work, we present BioWorkbench, a framework for managing and analyzing bioinformatics experiments. This framework automatically collects provenance data, including both performance data from workflow execution and data from the scientific domain of the workflow application. Provenance data can be analyzed through a web application that abstracts a set of queries to the provenance database, simplifying access to provenance information. We evaluate BioWorkbench using three case studies: SwiftPhylo, a phylogenetic tree assembly workflow; SwiftGECKO, a comparative genomics workflow; and RASflow, a RASopathy analysis workflow. We analyze each workflow from both computational and scientific domain perspectives, by using queries to a provenance and annotation database. Some of these queries are available as a pre-built feature of the BioWorkbench web application. Through the provenance data, we show that the framework is scalable and achieves high-performance, reducing up to 98{\%} of the case studies execution time. We also show how the application of machine learning techniques can enrich the analysis process.},
archivePrefix = {arXiv},
arxivId = {1801.03915},
author = {Mondelli, Maria Luiza and Magalh{\~{a}}es, Thiago and Loss, Guilherme and Wilde, Michael and Foster, Ian and Mattoso, Marta and Katz, Daniel S and Barbosa, Helio J C and Vasconcelos, Ana Tereza R. and Oca{\~{n}}a, Kary and Gadelha, Luiz M R},
eprint = {1801.03915},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mondelli et al. - 2018 - BioWorkbench A High-Performance Framework for Managing and Analyzing Bioinformatics Experiments.pdf:pdf},
pages = {1--20},
title = {{BioWorkbench: A High-Performance Framework for Managing and Analyzing Bioinformatics Experiments}},
url = {https://arxiv.org/pdf/1801.03915.pdf{\%}0Ahttp://arxiv.org/abs/1801.03915},
year = {2018}
}
@article{Moreau2015,
abstract = {The prov family of documents are the final output of the World Wide Web Consortium Provenance Working Group, chartered to specify a representation of provenance to facilitate its exchange over the Web. This article reflects upon the key requirements, guiding principles, and design decisions that influenced the prov family of documents. A broad range of requirements were found, relating to the key concepts necessary for describing provenance, such as resources, activities, agents and events, and to balancing prov's ease of use with the facility to check its validity. By this retrospective requirement analysis, the article aims to provide some insights into how prov turned out as it did and why. Benefits of this insight include better inter-operability, a roadmap for alternate investigations and improvements, and solid foundations for future standardization activities.},
author = {Moreau, Luc and Groth, Paul and Cheney, James and Lebo, Timothy and Miles, Simon},
doi = {10.1016/j.websem.2015.04.001},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moreau et al. - 2015 - The rationale of PROV.pdf:pdf},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Design decision,PROV,Provenance,Rationale,Requirement,Standardization,prov},
mendeley-tags = {PROV,Provenance},
pages = {235--257},
publisher = {Elsevier B.V.},
title = {{The rationale of PROV}},
url = {http://dx.doi.org/10.1016/j.websem.2015.04.001},
volume = {35},
year = {2015}
}
@incollection{Kohwalter2016,
abstract = {The World Wide Web evolves into a Web of Data, a huge, globally distributed dataspace that contains a rich body of machine-processable information from a virtually unbound set of providers covering a wide range of topics. However, due to the openness of the Web little is known about who created the data and how. The fact that a large amount of the data on the Web is derived by replication, query processing, modification, or merging raises concerns of information quality. Poor quality data may propagate quickly and contaminate the Web of Data. Provenance information about who created and published the data and how, provides the means for quality assessment. This paper takes a first step towards creating a quality-aware Web of Data: we present approaches to integrate provenance information into the Web of Data and we illustrate how this information can be consumed. In particular, we introduce a vocabulary to describe provenance of Web data as metadata and we discuss possibilities to make such provenance metadata accessible as part of the Web of Data. Furthermore, we describe how this metadata can be queried and consumed to identify outdated information.},
archivePrefix = {arXiv},
arxivId = {1406.2495},
author = {Kohwalter, Troy and Oliveira, Thiago and Freire, Juliana and Clua, Esteban and Murta, Leonardo},
doi = {10.1007/978-3-319-40593-3_6},
eprint = {1406.2495},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kohwalter et al. - 2016 - Prov Viewer A Graph-Based Visualization Tool for Interactive Exploration of Provenance Data.pdf:pdf},
isbn = {978-3-642-34221-9},
issn = {03029743},
keywords = {provenance,visualisation},
mendeley-tags = {provenance,visualisation},
pages = {71--82},
pmid = {9156564},
title = {{Prov Viewer: A Graph-Based Visualization Tool for Interactive Exploration of Provenance Data}},
url = {http://link.springer.com/10.1007/978-3-642-34222-6 http://link.springer.com/10.1007/978-3-319-40593-3{\_}6},
volume = {7525},
year = {2016}
}
@article{Cheney2011,
abstract = {Provenance is information recording the source, derivation, or history of some information. Provenance tracking has been studied in a variety of settings; however, although many design points have been explored, the mathematical or semantic foundations of data provenance have received comparatively little attention. In this paper, we argue that dependency analysis techniques familiar from program analysis and program slicing provide a formal foundation for forms of provenance that are intended to show how (part of) the output of a query depends on (parts of) its input. We introduce a semantic characterization of such dependency provenance, show that this form of provenance is not computable, and provide dynamic and static approximation techniques.},
archivePrefix = {arXiv},
arxivId = {arXiv:0708.2173v2},
author = {Cheney, James and Ahmed, Amal and Acar, Umut A.},
doi = {10.1017/S0960129511000211},
eprint = {arXiv:0708.2173v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheney, Ahmed, Acar - 2011 - Provenance as dependency analysis.pdf:pdf},
isbn = {978-3-540-75987-4},
issn = {09601295},
journal = {Mathematical Structures in Computer Science},
number = {6},
pages = {1301--1337},
title = {{Provenance as dependency analysis}},
volume = {21},
year = {2011}
}
@incollection{Stamatogiannakis2016,
abstract = {The World Wide Web evolves into a Web of Data, a huge, globally distributed dataspace that contains a rich body of machine-processable information from a virtually unbound set of providers covering a wide range of topics. However, due to the openness of the Web little is known about who created the data and how. The fact that a large amount of the data on the Web is derived by replication, query processing, modification, or merging raises concerns of information quality. Poor quality data may propagate quickly and contaminate the Web of Data. Provenance information about who created and published the data and how, provides the means for quality assessment. This paper takes a first step towards creating a quality-aware Web of Data: we present approaches to integrate provenance information into the Web of Data and we illustrate how this information can be consumed. In particular, we introduce a vocabulary to describe provenance of Web data as metadata and we discuss possibilities to make such provenance metadata accessible as part of the Web of Data. Furthermore, we describe how this metadata can be queried and consumed to identify outdated information.},
archivePrefix = {arXiv},
arxivId = {1406.2495},
author = {Stamatogiannakis, Manolis and Kazmi, Hasanat and Sharif, Hashim and Vermeulen, Remco and Gehani, Ashish and Bos, Herbert and Groth, Paul},
doi = {10.1007/978-3-319-40593-3_3},
eprint = {1406.2495},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stamatogiannakis et al. - 2016 - Trade-Offs in Automatic Provenance Capture.pdf:pdf},
isbn = {978-3-642-34221-9},
issn = {03029743},
keywords = {llvm,provenance,spade,strace,taint tracking},
pages = {29--41},
pmid = {9156564},
title = {{Trade-Offs in Automatic Provenance Capture}},
url = {http://link.springer.com/10.1007/978-3-642-34222-6 http://link.springer.com/10.1007/978-3-319-40593-3{\_}3},
volume = {7525},
year = {2016}
}
@article{Rosseti2017,
author = {Rosseti, Isabel and Oca{\~{n}}a, Kary A C S and Oliveira, Daniel De},
doi = {10.475/123},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rosseti, Oca{\~{n}}a, Oliveira - 2017 - Towards Preserving Results Confidentiality in Cloud-based Scientific Workflows.pdf:pdf},
isbn = {1234567245},
keywords = {all or part of,data distribution,data provenance,or,or hard copies of,permission to make digital,re-,scientific workflows,sults confidentiality,this work for personal},
title = {{Towards Preserving Results Confidentiality in Cloud-based Scientific Workflows}},
year = {2017}
}
@article{Hettne2012,
abstract = {In this position paper we present a set of best practices for workflow design to prevent workflow decay and increase reuse and re-purposing of scientific workflows. MyExperiment provides access to a large number of scientific workflows. However, scientists find it difficult to reuse or re-purpose these workflows for mainly two reasons: workflows suffer from decay over time and lack sufficient metadata to understand their purpose. We argue that good workflow design is a prerequisite for repairing a workflow, or redesigning an equivalent workflow pattern with new components. We present a set of best practices for workflow design and the semantic tooling that is being developed in the Workflow4Ever (Wf4Ever) project to support these best practices.},
author = {Hettne, Kristina and Wolstencroft, Katy and Belhajjame, Khalid and Goble, Carole and Mina, Eleni and Dharuri, Harish and Verdes-Montenegro, Lourdes and Garrido, Julian and {De Roure}, David and Roos, Marco},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hettne et al. - 2012 - Best practices for workflow design How to prevent workflow decay.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Best practices,Decay,Scientific workflows,workflow},
mendeley-tags = {workflow},
title = {{Best practices for workflow design: How to prevent workflow decay}},
volume = {952},
year = {2012}
}
